{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use-Case: Machine Learning based Leakage Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anomaly detection such as leakage detection is a classic but often non-trivial task in WDN operation. With traditional (model-based) methods reaching their limits, Machine Learning offers promising solutions.\n",
    "\n",
    "#### Outline \n",
    "This notebook demonstrates how EPyT-Flow and EPyT-Control can be utilized to create a scenario containing several leakages that have to be detected.\n",
    "Here, we use a simple Machine Learning based leakage detector that is already included in EPyT-Flow.\n",
    "It consists of the following steps:\n",
    "1. Create a new (realistic) scenario.\n",
    "2. Add some leakages to the scenario.\n",
    "3. Create a simple Machine Learning based leakage detector.\n",
    "4. Evaluate the leakage detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ImportWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import epyt_flow\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from epyt_flow.data.networks import load_ltown\n",
    "from epyt_flow.data.benchmarks import load_leakdb_scenarios\n",
    "from epyt_flow.simulation import ScenarioSimulator\n",
    "from epyt_flow.simulation.events import AbruptLeakage, IncipientLeakage\n",
    "from epyt_flow.utils import to_seconds, time_points_to_one_hot_encoding\n",
    "from epyt_control.signal_processing import SensorInterpolationDetector\n",
    "from joblib import Parallel, delayed, parallel_backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume that we know which specific nodes we want to watch:\n",
    "def calc_Relevance (node_pressures, relevant_index):\n",
    "    relevances = {}\n",
    "    for i in range(len(node_pressures)):\n",
    "        relevances[i] = mutual_info_score(node_pressures[relevant_index], node_pressures[i])\n",
    "    return relevances\n",
    "\n",
    "# calcuate relevance over all nodes:\n",
    "\n",
    "def calc_Relevances (node_pressures):\n",
    "    relevances = {}\n",
    "    for i in range(len(node_pressures)):\n",
    "        for j in range(i + 1, len(node_pressures)):\n",
    "            m_i_score = mutual_info_score(node_pressures[i], node_pressures[j])\n",
    "            if i not in relevances:\n",
    "                relevances[i] = m_i_score\n",
    "            else:\n",
    "                relevances[i] += m_i_score\n",
    "            if j not in relevances:\n",
    "                relevances[j] = m_i_score\n",
    "            else:\n",
    "                relevances[j] += m_i_score\n",
    "    return relevances\n",
    "    \n",
    "def calc_Redundance (x_pressures, sensor_pressures):\n",
    "    redundancy = 0\n",
    "    for pressure in sensor_pressures:\n",
    "        redundancy += mutual_info_score(x_pressures, pressure)\n",
    "    redundancy /= len(sensor_pressures)\n",
    "    return redundancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_Relevances_parallel(node_pressures, n_jobs=1):\n",
    "    relevances = Parallel(n_jobs=n_jobs)(delayed(calc_Relevance)(node_pressures, i) for i in range(len(node_pressures)))\n",
    "    return {i: sum(relevance[i] for relevance in relevances) for i in range(len(node_pressures))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_sensor_placement(node_pressures, num_sensors = 15): # do we know which nodes we want to monitor? junctions? all of them?\n",
    "    remaining_nodes = list(range(len(node_pressures)))\n",
    "    relevances = calc_Relevances(node_pressures)  # Assume we want to know the relevances in regard to all nodes\n",
    "    Sensor_placement = [max(relevances, key=relevances.get)]\n",
    "    remaining_nodes.remove(Sensor_placement[-1])\n",
    "    sensor_pressures = [node_pressures[Sensor_placement[-1]]]\n",
    "    no_redundance_nodes = [a for a in remaining_nodes if calc_Redundance(node_pressures[a], node_pressures[Sensor_placement]) == 0]\n",
    "    while no_redundance_nodes: # add nodes of highest relevance without any redundance\n",
    "        if (len(Sensor_placement) >= num_sensors):\n",
    "            break\n",
    "        tmp_relevances = relevances.copy()\n",
    "        for i in range(len(tmp_relevances.keys())): # remove nodes that have been added to the sensors and nodes with redundance\n",
    "            if i in Sensor_placement:\n",
    "                del tmp_relevances[i]\n",
    "                continue\n",
    "            if i not in no_redundance_nodes:\n",
    "                del tmp_relevances[i]\n",
    "        Sensor_placement.append(max(tmp_relevances, key=relevances.get))\n",
    "        remaining_nodes.remove(Sensor_placement[-1])\n",
    "        sensor_pressures.append(node_pressures[Sensor_placement[-1]])\n",
    "        no_redundance_nodes = [a for a in remaining_nodes if calc_Redundance(node_pressures[a], sensor_pressures) == 0]\n",
    "    remaining_relevances = list(map(relevances.get, remaining_nodes))\n",
    "    while not(all(list(map(lambda x: x == 0, remaining_relevances)))) and remaining_nodes:\n",
    "        if (len(Sensor_placement) >= num_sensors):\n",
    "            break\n",
    "        RRI = {}\n",
    "        for i in remaining_nodes:\n",
    "            RRI[i] = relevances[i]/ calc_Redundance(node_pressures[i], sensor_pressures)\n",
    "        Sensor_placement.append(max(RRI, key=RRI.get))\n",
    "        sensor_pressures.append(node_pressures[Sensor_placement[-1]])\n",
    "        remaining_nodes.remove(Sensor_placement[-1])\n",
    "        remaining_relevances = list(map(relevances.get, remaining_nodes))\n",
    "    return Sensor_placement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_sensor_placement_parallel(node_pressures, num_sensors = 15, n_jobs = 1): # do we know which nodes we want to monitor? junctions? all of them?\n",
    "    remaining_nodes = list(range(len(node_pressures)))\n",
    "    if n_jobs > 1:\n",
    "        relevances = calc_Relevances_parallel(node_pressures, n_jobs=n_jobs)\n",
    "    else:\n",
    "        relevances = calc_Relevances(node_pressures)  # Assume we want to know the relevances in regard to all nodes\n",
    "    Sensor_placement = [max(relevances, key=relevances.get)]\n",
    "    remaining_nodes.remove(Sensor_placement[-1])\n",
    "    sensor_pressures = [node_pressures[Sensor_placement[-1]]]\n",
    "    Redundances = Parallel(n_jobs=n_jobs)(delayed(calc_Redundance)(node_pressures[a], node_pressures[Sensor_placement]) for a in remaining_nodes)\n",
    "    no_redundance_nodes = [remaining_nodes[a] for a in range(len(remaining_nodes)) if Redundances[a] == 0]\n",
    "    while no_redundance_nodes: # add nodes of highest relevance without any redundance\n",
    "        if (len(Sensor_placement) >= num_sensors):\n",
    "            break\n",
    "        tmp_relevances = relevances.copy()\n",
    "        for i in range(len(tmp_relevances.keys())): # remove nodes that have been added to the sensors and nodes with redundance\n",
    "            if i in Sensor_placement:\n",
    "                del tmp_relevances[i]\n",
    "                continue\n",
    "            if i not in no_redundance_nodes:\n",
    "                del tmp_relevances[i]\n",
    "        Sensor_placement.append(max(tmp_relevances, key=relevances.get))\n",
    "        remaining_nodes.remove(Sensor_placement[-1])\n",
    "        sensor_pressures.append(node_pressures[Sensor_placement[-1]])\n",
    "        Redundances = Parallel(n_jobs=n_jobs)(delayed(calc_Redundance)(node_pressures[a], node_pressures[Sensor_placement]) for a in remaining_nodes)\n",
    "        no_redundance_nodes = [remaining_nodes[a] for a in range(len(remaining_nodes)) if Redundances[a] == 0]\n",
    "    remaining_relevances = list(map(relevances.get, remaining_nodes))\n",
    "    while not(all(list(map(lambda x: x == 0, remaining_relevances)))) and remaining_nodes:\n",
    "        if (len(Sensor_placement) >= num_sensors):\n",
    "            break\n",
    "        RRI = {}\n",
    "        for i in remaining_nodes:\n",
    "            RRI[i] = relevances[i]/ calc_Redundance(node_pressures[i], sensor_pressures)\n",
    "        Sensor_placement.append(max(RRI, key=RRI.get))\n",
    "        sensor_pressures.append(node_pressures[Sensor_placement[-1]])\n",
    "        remaining_nodes.remove(Sensor_placement[-1])\n",
    "        remaining_relevances = list(map(relevances.get, remaining_nodes))\n",
    "    return Sensor_placement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DecisionTreeClassifier with entropy criterion is in the ID3 style. TODO: Might use decision-tree-id3 package if it works?\n",
    "def mixed_model_classification_fit(n_estimators, X_train, y_train, n_jobs=1):\n",
    "    with parallel_backend('threading', n_jobs=n_jobs):\n",
    "        base = DecisionTreeClassifier(criterion='entropy') #Entropy criterion should be approximate to ID3 decision tree. Sadly no official ID3 Implementation is given\n",
    "        bagged = BaggingClassifier(estimator=base, n_estimators=n_estimators, max_samples=0.8, oob_score=True) #for faster runtime adjust parameters, especially n_estimators\n",
    "        bagged.fit(X_train, y_train)\n",
    "    return bagged\n",
    "\n",
    "def mixed_model_classification_predict(classifier, X_test, y_test, n_jobs=1):\n",
    "    with parallel_backend('threading', n_jobs=n_jobs):\n",
    "        # Using the classifier to predict the labels for the test set\n",
    "        # This will also print the classification report\n",
    "        y_pred = classifier.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred, labels=np.unique(y_test),\n",
    "                                target_names=[f\"class_{int(c)}\" for c in np.unique(y_test)], output_dict=False))\n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = epyt_flow.data.benchmarks.leakdb.load_data(\n",
    "    scenarios_id=[\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\"],  \n",
    "    #scenarios_id=[\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\"],\n",
    "    use_net1=False,\n",
    "    return_X_y=True\n",
    ")\n",
    "\n",
    "X_list, y_list = [], [] \n",
    "for sid, (Xi, yi) in data_dict.items():\n",
    "    X_list.append(Xi.values if hasattr(Xi, \"values\") else Xi) #Check for correctness. \n",
    "    y_list.append(yi)\n",
    "\n",
    "X = np.vstack(X_list)        # shape → (total_samples, n_features)\n",
    "y = np.concatenate(y_list)   # shape → (total_samples,)\n",
    "\n",
    "print(f\"Total samples: {X.shape[0]}, features: {X.shape[1]}\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( #Might use datasets given train-test split\n",
    "    X, y,\n",
    "    test_size=0.20,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "clf = mixed_model_classification_fit(100, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create new Scenario\n",
    "\n",
    "Create a new scenario based on the [L-Town network](https://epyt-flow.readthedocs.io/en/stable/epyt_flow.data.html#epyt_flow.data.networks.load_ltown) with a default sensor configuration and realistic demand patterns from the [BattLeDIM challenge](https://battledim.ucy.ac.cy/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: adapt to possibility of using battledim\n",
    "#NOTE: this approach calculates a sensor placement based on multiple scenarios, i.e. all scenarios we look at\n",
    "#      due to the complexity of the algorithm this will take a lot of time!\n",
    "scen_train_keys = [str(num) for num in random.sample(range(1, 501), 10)]\n",
    "scen_test_keys = [\"511\", \"512\", \"513\", \"514\", \"515\", \"516\", \"517\", \"518\", \"519\", \"520\"]\n",
    "scen_keys = scen_train_keys + scen_test_keys\n",
    "data = epyt_flow.data.benchmarks.leakdb.load_data(scenarios_id=scen_train_keys, use_net1=False, return_X_y=False)\n",
    "\n",
    "pressure_column_heads = data[scen_train_keys[0]].columns[:32]\n",
    "node_pressures = []\n",
    "for key in scen_train_keys:\n",
    "    for i in range(len(pressure_column_heads)):\n",
    "        if key == scen_train_keys[0]:\n",
    "            node_pressures.append(data[key]['Pressure-Node_' + str(i + 1)])\n",
    "        else:\n",
    "            node_pressures[i] = np.concatenate((node_pressures[i], data[key]['Pressure-Node_' + str(i + 1)]))\n",
    "node_pressures = np.array(node_pressures)\n",
    "node_pressures.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_sensor_placements = calc_sensor_placement_parallel(node_pressures, num_sensors=15, n_jobs=4)\n",
    "sensor_placements = calc_sensor_placement(node_pressures, num_sensors=15)\n",
    "print(\"Parallel sensor placements:\", parallel_sensor_placements)\n",
    "print(\"Sensor placements:\", sensor_placements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: adapt to possible changes in the dataset\n",
    "configs = load_leakdb_scenarios(scenarios_id=scen_keys, use_net1=False, verbose=True)\n",
    "scenarios = {}\n",
    "for i, key in enumerate(scen_keys):\n",
    "    scenarios[key] = ScenarioSimulator(scenario_config=configs[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set simulation duration to 2 weeks and use 5min time intervals for the hydraulics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios[scen_train_keys[0]].plot_topology()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Add Leakages to the Scenario\n",
    "\n",
    "In this example, we build a scenario with two leakages: A small abrupt leakage and a large incipient leakage in the second week:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the complete simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scada_data = {}\n",
    "for key, scenario in scenarios.items():\n",
    "    scada_data[key] = scenario.run_simulation(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Machine Learning based Leakage Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the simulation results for calibrating (i.e. creating) a Machine Learning based leakage detection method:\n",
    "\n",
    "- Create a feature vector (pressure and flow readings at the sensors).\n",
    "- Create ground-truth labels utilizing the [`time_points_to_one_hot_encoding()`](https://epyt-flow.readthedocs.io/en/stable/epyt_flow.html#epyt_flow.utils.time_points_to_one_hot_encoding) helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate pressure and flow readings into a single feature vector\n",
    "# X = np.concatenate((scada_data.get_data_pressures(), scada_data.get_data_flows()), axis=1)\n",
    "\"\"\"X_tmp = [scada_data[key].get_data_pressures()[:, sensor_placements] for key in scen_keys]\n",
    "X = np.vstack(X_tmp)\n",
    "y_tmp = []\n",
    "for key in scen_keys:\n",
    "    events_times = [int(t / configs[int(key)].general_params[\"hydraulic_time_step\"])\n",
    "                    for t in scenarios[key].get_events_active_time_points()]\n",
    "    y_tmp.append(time_points_to_one_hot_encoding(events_times, total_length=X_tmp[0].shape[0]))\n",
    "y = np.concatenate(y_tmp)\n",
    "print(f\"Total samples: {X.shape[0]}, features: {X.shape[1]}, samples_y : {y.shape[0]}\")\"\"\"\n",
    "X_tmp = [scada_data[key].get_data_pressures()[:, sensor_placements] for key in scen_train_keys]\n",
    "X_train = np.vstack(X_tmp)\n",
    "X_test = {key: scada_data[key].get_data_pressures()[:, sensor_placements] for key in scen_test_keys}\n",
    "y_tmp = []\n",
    "y_test = {}\n",
    "for i, key in enumerate(scen_train_keys):\n",
    "    events_times = [int(t / configs[i].general_params[\"hydraulic_time_step\"])\n",
    "                    for t in scenarios[key].get_events_active_time_points()]\n",
    "    y_tmp.append(time_points_to_one_hot_encoding(events_times, total_length=X_tmp[0].shape[0]))\n",
    "for i, key in enumerate(scen_test_keys):\n",
    "    events_times = [int(t / configs[i].general_params[\"hydraulic_time_step\"])\n",
    "                    for t in scenarios[key].get_events_active_time_points()]\n",
    "    y_test[key] = time_points_to_one_hot_encoding(events_times, total_length=X_test[key].shape[0])\n",
    "y_train = np.concatenate(y_tmp)\n",
    "print(f\"Total samples: {X_train.shape[0]}, features: {X_train.shape[1]}, samples_y : {y_train.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the following three"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into train and test set -- here, training data is the fault-free first week of the simulation and the second week (containing the leakages) is the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.20,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\"\"\"\n",
    "\n",
    "\n",
    "X_train, y_train = sklearn.utils.shuffle(\n",
    "    X_train, y_train,\n",
    "    random_state=42\n",
    ")\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Machine Learning based Event Detector\n",
    "\n",
    "\n",
    "As a classic baseline, EPyT-Flow already implements a residual-based interpolation detection method called [`SensorInterpolationDetector`](https://epyt-flow.readthedocs.io/en/stable/epyt_flow.models.html#epyt_flow.models.sensor_interpolation_detector.SensorInterpolationDetector).\n",
    "\n",
    "This method tries to predict the readings of a given sensor based on all other sensors: $f: \\vec{x}_t\\setminus\\{i\\} \\mapsto (\\vec{x}_t)_i$, where $\\vec{x}_t$ refers to these sensor ratings at time $t$, and $\\vec{x}_t\\setminus\\{i\\}$ denotes these sensor readings without the $i$-th sensor.\n",
    "An alarm is raised (i.e. event detected) whenever the prediction and the observation of at least one sensor differ significantly:\n",
    "$$\n",
    "   \\exists i:\\; |f(\\vec{x}_t\\setminus\\{i\\}) - (\\vec{x}_t)_i| > \\theta_i\n",
    "$$\n",
    "where $\\theta_i > 0$ denotes a sensor-specific threshold at which the difference is considered as significant.\n",
    "For this, the detection method has to be calibrated (i.e. fitted) to a time window of (ideally event-free) sensor readings to determine a suitable threshold $\\theta$ that does not raise an alarm when the network is in normal operation (i.e. no events present).\n",
    "\n",
    "We use this event detector to detect leakages in our generated scenario.\n",
    "We create and calibrate (i.e. fit) the leakage detector to the first week of simulated data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = SensorInterpolationDetector()\n",
    "mixed_clf = mixed_model_classification_fit(100, X_train, y_train)\n",
    "\n",
    "detector.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the detector to the test data (i.e. second week of simulated data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#suspicious_time_points = detector.apply(X_test)\n",
    "suspicious_time_points = {key: mixed_model_classification_predict(mixed_clf, X_test[key], y_test[key]) for key in scen_test_keys}\n",
    "#suspicious_time_points_train = {key: mixed_model_classification_predict(mixed_clf, X_tmp[i], y_tmp[i]) for i, key in enumerate(scen_train_keys)}\n",
    "#suspicious_time_points = {i: mixed_model_classification_predict(mixed_clf, X_tmp[i], y_tmp[i]) for i in range(len(X_tmp))} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluation\n",
    "\n",
    "In order to evaluate the performance of the leakage detector, we could either compute the [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) or plot the raised alarms together with the ground truth labels.\n",
    "\n",
    "Here, we plot event (i.e. leakage) presence over time together with the raised alarms by the detector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_scenario = 6\n",
    "plt.figure()\n",
    "plt.plot(list(range(len(y_tmp[disp_scenario - 1]))), y_tmp[disp_scenario - 1], color=\"red\", label=\"Ground truth\")\n",
    "plt.bar(list(range(len(suspicious_time_points[disp_scenario - 1]))), suspicious_time_points[disp_scenario - 1], label=\"Raised alarm\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"Leakage indicator\")\n",
    "plt.yticks([0, 1], [\"Inactive\", \"Active\"])\n",
    "plt.xlabel(\"Time (5min steps)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate plots for test scenarios\n",
    "for key in scen_test_keys:\n",
    "    plt.figure()\n",
    "    plt.plot(list(range(len(y_test[key]))), y_test[key], color=\"red\", label=\"Ground truth\")\n",
    "    plt.bar(list(range(len(suspicious_time_points[key]))), suspicious_time_points[key], label=\"Raised alarm\")\n",
    "    plt.title(f\"Test Scenario {key}\")\n",
    "    plt.legend()\n",
    "    plt.ylabel(\"Leakage indicator\")\n",
    "    plt.yticks([0, 1], [\"Inactive\", \"Active\"])\n",
    "    plt.xlabel(\"Time (5min steps)\")\n",
    "    plt.show()\n",
    "\n",
    "\"\"\"# Generate plots for training scenarios\n",
    "for i, key in enumerate(scen_train_keys):\n",
    "    plt.figure()\n",
    "    plt.plot(list(range(len(y_tmp[i]))), y_tmp[i], color=\"red\", label=\"Ground truth\")\n",
    "    plt.bar(list(range(len(suspicious_time_points_train[key]))), suspicious_time_points_train[key], label=\"Raised alarm\")\n",
    "    plt.title(f\"Training Scenario {key}\")\n",
    "    plt.legend()\n",
    "    plt.ylabel(\"Leakage indicator\")\n",
    "    plt.yticks([0, 1], [\"Inactive\", \"Active\"])\n",
    "    plt.xlabel(\"Time (5min steps)\")\n",
    "    plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the small abrupt leakage is not detected, while the large incipient leakage is detected -- only a single false alarm is raised.\n",
    "\n",
    "**Note:** More advanced algorithms & methods are likely to show a better detection performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close the Simulation\n",
    "\n",
    "Do not forget to close the simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for scenario in scenarios.values():\n",
    "    scenario.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
