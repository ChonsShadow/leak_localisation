{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Pressure sensor placement for leakage detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "(This notebook is based on the epyt-flow tutorial for leakage detection found here: [Tutorials](https://github.com/WaterFutures/EPyT-and-EPyT-Flow-Tutorial))\n",
    "\n",
    "**Authors:** Timo Beckmann, Tobias Kroecker, Yasmine Marzouk\n",
    "\n",
    "This notebook documents a research project focused on evaluating two critical aspects of leakage detection in water distribution networks:\n",
    "\n",
    "1. **Sensor Placement Strategy**: We implement and evaluate a sensor placement algorithm based on information theory. This approach aims to optimize the placement of pressure sensors by maximizing relevance and minimizing redundancy, ensuring effective network coverage. The evaluation includes varying the number of sensors to analyze the impact of sensor density on detection performance.\n",
    "\n",
    "2. **Leakage Detection Method**: A machine learning-based leakage detection approach is implemented using a bagging classifier with decision trees. This method leverages ensemble learning to enhance robustness and accuracy in detecting anomalies such as leakages in the network.\n",
    "\n",
    "3. **Datasets**: \n",
    "    - **LeakDB**: A benchmark dataset providing realistic scenarios for leakage detection in water distribution networks, used to evaluate both the sensor placement strategy and the leakage detection method.\n",
    "    - **BattLeDIM**: A dataset offering realistic demand patterns and scenarios, used to simulate and validate the sensor placement strategy.\n",
    "\n",
    "4. **Evaluation Goals**: The primary objective is to assess the performance of the leakage detection method and the effectiveness of the sensor placement strategy. This includes:\n",
    "    - Analyzing the detection accuracy of the machine learning model.\n",
    "    - Investigating the influence of different sensor placement sizes on detection performance.\n",
    "\n",
    "By combining theoretical insights from information theory and machine learning with practical applications, this notebook provides a comprehensive evaluation of the interplay between sensor placement and leakage detection in water distribution networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and method definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ImportWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import epyt_flow\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from epyt_flow.data.networks import load_ltown\n",
    "from epyt_flow.data.benchmarks import load_leakdb_scenarios\n",
    "from epyt_flow.simulation import ScenarioSimulator\n",
    "from epyt_flow.simulation.events import AbruptLeakage, IncipientLeakage\n",
    "from epyt_flow.utils import to_seconds, time_points_to_one_hot_encoding\n",
    "from epyt_control.signal_processing import SensorInterpolationDetector\n",
    "from joblib import Parallel, delayed, parallel_backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of the sensor placement algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume that we know which specific nodes we want to watch:\n",
    "def calc_Relevance (node_pressures, relevant_index):\n",
    "    relevances = {}\n",
    "    for i in range(len(node_pressures)):\n",
    "        relevances[i] = mutual_info_score(node_pressures[relevant_index], node_pressures[i])\n",
    "    return relevances\n",
    "\n",
    "# calcuate relevance over all nodes:\n",
    "\n",
    "def calc_Relevances (node_pressures):\n",
    "    relevances = {}\n",
    "    for i in range(len(node_pressures)):\n",
    "        for j in range(i + 1, len(node_pressures)):\n",
    "            m_i_score = mutual_info_score(node_pressures[i], node_pressures[j])\n",
    "            if i not in relevances:\n",
    "                relevances[i] = m_i_score\n",
    "            else:\n",
    "                relevances[i] += m_i_score\n",
    "            if j not in relevances:\n",
    "                relevances[j] = m_i_score\n",
    "            else:\n",
    "                relevances[j] += m_i_score\n",
    "    return relevances\n",
    "    \n",
    "def calc_Redundance (x_pressures, sensor_pressures):\n",
    "    redundancy = 0\n",
    "    for pressure in sensor_pressures:\n",
    "        redundancy += mutual_info_score(x_pressures, pressure)\n",
    "    redundancy /= len(sensor_pressures)\n",
    "    return redundancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_Relevances_parallel(node_pressures, n_jobs=1):\n",
    "    relevances = Parallel(n_jobs=n_jobs)(delayed(calc_Relevance)(node_pressures, i) for i in range(len(node_pressures)))\n",
    "    return {i: sum(relevance[i] for relevance in relevances) for i in range(len(node_pressures))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_sensor_placement(node_pressures, num_sensors = 15): # do we know which nodes we want to monitor? junctions? all of them?\n",
    "    remaining_nodes = list(range(len(node_pressures)))\n",
    "    relevances = calc_Relevances(node_pressures)  # Assume we want to know the relevances in regard to all nodes\n",
    "    Sensor_placement = [max(relevances, key=relevances.get)]\n",
    "    remaining_nodes.remove(Sensor_placement[-1])\n",
    "    sensor_pressures = [node_pressures[Sensor_placement[-1]]]\n",
    "    no_redundance_nodes = [a for a in remaining_nodes if calc_Redundance(node_pressures[a], node_pressures[Sensor_placement]) == 0]\n",
    "    while no_redundance_nodes: # add nodes of highest relevance without any redundance\n",
    "        if (len(Sensor_placement) >= num_sensors):\n",
    "            break\n",
    "        tmp_relevances = relevances.copy()\n",
    "        for i in range(len(tmp_relevances.keys())): # remove nodes that have been added to the sensors and nodes with redundance\n",
    "            if i in Sensor_placement:\n",
    "                del tmp_relevances[i]\n",
    "                continue\n",
    "            if i not in no_redundance_nodes:\n",
    "                del tmp_relevances[i]\n",
    "        Sensor_placement.append(max(tmp_relevances, key=relevances.get))\n",
    "        remaining_nodes.remove(Sensor_placement[-1])\n",
    "        sensor_pressures.append(node_pressures[Sensor_placement[-1]])\n",
    "        no_redundance_nodes = [a for a in remaining_nodes if calc_Redundance(node_pressures[a], sensor_pressures) == 0]\n",
    "    remaining_relevances = list(map(relevances.get, remaining_nodes))\n",
    "    while not(all(list(map(lambda x: x == 0, remaining_relevances)))) and remaining_nodes:\n",
    "        if (len(Sensor_placement) >= num_sensors):\n",
    "            break\n",
    "        RRI = {}\n",
    "        for i in remaining_nodes:\n",
    "            RRI[i] = relevances[i]/ calc_Redundance(node_pressures[i], sensor_pressures)\n",
    "        Sensor_placement.append(max(RRI, key=RRI.get))\n",
    "        sensor_pressures.append(node_pressures[Sensor_placement[-1]])\n",
    "        remaining_nodes.remove(Sensor_placement[-1])\n",
    "        remaining_relevances = list(map(relevances.get, remaining_nodes))\n",
    "    return Sensor_placement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_sensor_placement_parallel(node_pressures, num_sensors = 15, n_jobs = 1): # do we know which nodes we want to monitor? junctions? all of them?\n",
    "    remaining_nodes = list(range(len(node_pressures)))\n",
    "    if n_jobs > 1:\n",
    "        relevances = calc_Relevances_parallel(node_pressures, n_jobs=n_jobs)\n",
    "    else:\n",
    "        relevances = calc_Relevances(node_pressures)  # Assume we want to know the relevances in regard to all nodes\n",
    "    Sensor_placement = [max(relevances, key=relevances.get)]\n",
    "    remaining_nodes.remove(Sensor_placement[-1])\n",
    "    sensor_pressures = [node_pressures[Sensor_placement[-1]]]\n",
    "    Redundances = Parallel(n_jobs=n_jobs)(delayed(calc_Redundance)(node_pressures[a], node_pressures[Sensor_placement]) for a in remaining_nodes)\n",
    "    no_redundance_nodes = [remaining_nodes[a] for a in range(len(remaining_nodes)) if Redundances[a] == 0]\n",
    "    while no_redundance_nodes: # add nodes of highest relevance without any redundance\n",
    "        if (len(Sensor_placement) >= num_sensors):\n",
    "            break\n",
    "        tmp_relevances = relevances.copy()\n",
    "        for i in range(len(tmp_relevances.keys())): # remove nodes that have been added to the sensors and nodes with redundance\n",
    "            if i in Sensor_placement:\n",
    "                del tmp_relevances[i]\n",
    "                continue\n",
    "            if i not in no_redundance_nodes:\n",
    "                del tmp_relevances[i]\n",
    "        Sensor_placement.append(max(tmp_relevances, key=relevances.get))\n",
    "        remaining_nodes.remove(Sensor_placement[-1])\n",
    "        sensor_pressures.append(node_pressures[Sensor_placement[-1]])\n",
    "        Redundances = Parallel(n_jobs=n_jobs)(delayed(calc_Redundance)(node_pressures[a], node_pressures[Sensor_placement]) for a in remaining_nodes)\n",
    "        no_redundance_nodes = [remaining_nodes[a] for a in range(len(remaining_nodes)) if Redundances[a] == 0]\n",
    "    remaining_relevances = list(map(relevances.get, remaining_nodes))\n",
    "    while not(all(list(map(lambda x: x == 0, remaining_relevances)))) and remaining_nodes:\n",
    "        if (len(Sensor_placement) >= num_sensors):\n",
    "            break\n",
    "        RRI = {}\n",
    "        for i in remaining_nodes:\n",
    "            RRI[i] = relevances[i]/ calc_Redundance(node_pressures[i], sensor_pressures)\n",
    "        Sensor_placement.append(max(RRI, key=RRI.get))\n",
    "        sensor_pressures.append(node_pressures[Sensor_placement[-1]])\n",
    "        remaining_nodes.remove(Sensor_placement[-1])\n",
    "        remaining_relevances = list(map(relevances.get, remaining_nodes))\n",
    "    return Sensor_placement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of the bagging classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DecisionTreeClassifier with entropy criterion is in the ID3 style. TODO: Might use decision-tree-id3 package if it works?\n",
    "def mixed_model_classification_fit(n_estimators, X_train, y_train, n_jobs=1):\n",
    "    with parallel_backend('threading', n_jobs=n_jobs):\n",
    "        base = DecisionTreeClassifier(criterion='entropy') #Entropy criterion should be approximate to ID3 decision tree. Sadly no official ID3 Implementation is given\n",
    "        bagged = BaggingClassifier(estimator=base, n_estimators=n_estimators, max_samples=0.8, oob_score=True) #for faster runtime adjust parameters, especially n_estimators\n",
    "        bagged.fit(X_train, y_train)\n",
    "    return bagged\n",
    "\n",
    "def mixed_model_classification_predict(classifier, X_test, y_test, n_jobs=1):\n",
    "    with parallel_backend('threading', n_jobs=n_jobs):\n",
    "        # Using the classifier to predict the labels for the test set\n",
    "        # This will also print the classification report\n",
    "        y_pred = classifier.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred, labels=np.unique(y_test),\n",
    "                                target_names=[f\"class_{int(c)}\" for c in np.unique(y_test)], output_dict=False))\n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the classifier:\n",
    "data_dict = epyt_flow.data.benchmarks.leakdb.load_data(\n",
    "    scenarios_id=[\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\"],  \n",
    "    #scenarios_id=[\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\"],\n",
    "    use_net1=False,\n",
    "    return_X_y=True\n",
    ")\n",
    "\n",
    "X_list, y_list = [], [] \n",
    "for sid, (Xi, yi) in data_dict.items():\n",
    "    X_list.append(Xi.values if hasattr(Xi, \"values\") else Xi) #Check for correctness. \n",
    "    y_list.append(yi)\n",
    "\n",
    "X = np.vstack(X_list)        # shape → (total_samples, n_features)\n",
    "y = np.concatenate(y_list)   # shape → (total_samples,)\n",
    "\n",
    "print(f\"Total samples: {X.shape[0]}, features: {X.shape[1]}\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( #Might use datasets given train-test split\n",
    "    X, y,\n",
    "    test_size=0.20,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "clf = mixed_model_classification_fit(100, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_model_classification_predict(clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeakDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Loading scenarios and placing sensors\n",
    "\n",
    "In this part a subset of the scenarios from LeakDB is loaded and based on existiong pressure data from the different scenarios, the sensor placements are calculated. The scenarios are split into a train and a testset, based on their ids. The pressure sensors are placed based on the training data, assuming that in a realistic scenario the sensors could only be placed based on a finite set of pressure data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: adapt to possibility of using battledim\n",
    "#NOTE: this approach calculates a sensor placement based on multiple scenarios, i.e. all scenarios we look at\n",
    "#      due to the complexity of the algorithm this will take a lot of time!\n",
    "scen_train_keys = [str(num) for num in random.sample(range(1, 501), 10)]\n",
    "scen_test_keys = [\"511\", \"512\", \"513\", \"514\", \"515\", \"516\", \"517\", \"518\", \"519\", \"520\"]\n",
    "scen_keys = scen_train_keys + scen_test_keys\n",
    "data = epyt_flow.data.benchmarks.leakdb.load_data(scenarios_id=scen_train_keys, use_net1=False, return_X_y=False)\n",
    "\n",
    "pressure_column_heads = data[scen_train_keys[0]].columns[:32]\n",
    "node_pressures = []\n",
    "for key in scen_train_keys:\n",
    "    for i in range(len(pressure_column_heads)):\n",
    "        if key == scen_train_keys[0]:\n",
    "            node_pressures.append(data[key]['Pressure-Node_' + str(i + 1)])\n",
    "        else:\n",
    "            node_pressures[i] = np.concatenate((node_pressures[i], data[key]['Pressure-Node_' + str(i + 1)]))\n",
    "node_pressures = np.array(node_pressures)\n",
    "node_pressures.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_sensor_placements = calc_sensor_placement_parallel(node_pressures, num_sensors=15, n_jobs=4)\n",
    "sensor_placements = calc_sensor_placement(node_pressures, num_sensors=15)\n",
    "print(\"Parallel sensor placements:\", parallel_sensor_placements)\n",
    "print(\"Sensor placements:\", sensor_placements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = load_leakdb_scenarios(scenarios_id=scen_keys, use_net1=False, verbose=True)\n",
    "scenarios = {}\n",
    "for i, key in enumerate(scen_keys):\n",
    "    scenarios[key] = ScenarioSimulator(scenario_config=configs[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The topology of the water distribution network does not change between the different scenarios. Here the general topology is inspected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios[scen_train_keys[0]].plot_topology()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the complete set of simulations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scada_data = {}\n",
    "for key, scenario in scenarios.items():\n",
    "    scada_data[key] = scenario.run_simulation(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning based Leakage Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the simulation results for calibrating (i.e. creating) a Machine Learning based leakage detection method:\n",
    "\n",
    "- Create a feature vector (pressure readings at the sensors).\n",
    "- Create ground-truth labels utilizing the [`time_points_to_one_hot_encoding()`](https://epyt-flow.readthedocs.io/en/stable/epyt_flow.html#epyt_flow.utils.time_points_to_one_hot_encoding) helper function.\n",
    "\n",
    "The scenarios are already split into a train and test set. Because of that, two feature vectors and lable-sets are created from the start, one of each for the training of the classifier and another pair for the testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate pressure and flow readings into a single feature vector\n",
    "# X = np.concatenate((scada_data.get_data_pressures(), scada_data.get_data_flows()), axis=1)\n",
    "\"\"\"X_tmp = [scada_data[key].get_data_pressures()[:, sensor_placements] for key in scen_keys]\n",
    "X = np.vstack(X_tmp)\n",
    "y_tmp = []\n",
    "for key in scen_keys:\n",
    "    events_times = [int(t / configs[int(key)].general_params[\"hydraulic_time_step\"])\n",
    "                    for t in scenarios[key].get_events_active_time_points()]\n",
    "    y_tmp.append(time_points_to_one_hot_encoding(events_times, total_length=X_tmp[0].shape[0]))\n",
    "y = np.concatenate(y_tmp)\n",
    "print(f\"Total samples: {X.shape[0]}, features: {X.shape[1]}, samples_y : {y.shape[0]}\")\"\"\"\n",
    "X_tmp = [scada_data[key].get_data_pressures()[:, sensor_placements] for key in scen_train_keys]\n",
    "X_train = np.vstack(X_tmp)\n",
    "X_test = {key: scada_data[key].get_data_pressures()[:, sensor_placements] for key in scen_test_keys}\n",
    "y_tmp = []\n",
    "y_test = {}\n",
    "for i, key in enumerate(scen_train_keys):\n",
    "    events_times = [int(t / configs[i].general_params[\"hydraulic_time_step\"])\n",
    "                    for t in scenarios[key].get_events_active_time_points()]\n",
    "    y_tmp.append(time_points_to_one_hot_encoding(events_times, total_length=X_tmp[0].shape[0]))\n",
    "for i, key in enumerate(scen_test_keys):\n",
    "    events_times = [int(t / configs[i].general_params[\"hydraulic_time_step\"])\n",
    "                    for t in scenarios[key].get_events_active_time_points()]\n",
    "    y_test[key] = time_points_to_one_hot_encoding(events_times, total_length=X_test[key].shape[0])\n",
    "y_train = np.concatenate(y_tmp)\n",
    "print(f\"Total samples: {X_train.shape[0]}, features: {X_train.shape[1]}, samples_y : {y_train.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data is shuffled to avoid biases: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.20,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\"\"\"\n",
    "\n",
    "\n",
    "X_train, y_train = sklearn.utils.shuffle(\n",
    "    X_train, y_train,\n",
    "    random_state=42\n",
    ")\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_clf = mixed_model_classification_fit(100, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the classifier to the test data (i.e. the test scenarios). Note that one-hot-encoding isn't necessary here, since our classifier is already trained to return one prediction, 0 or 1, per timestep:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suspicious_time_points = {key: mixed_model_classification_predict(mixed_clf, X_test[key], y_test[key]) for key in scen_test_keys}\n",
    "#suspicious_time_points_train = {key: mixed_model_classification_predict(mixed_clf, X_tmp[i], y_tmp[i]) for i, key in enumerate(scen_train_keys)}\n",
    "#suspicious_time_points = {i: mixed_model_classification_predict(mixed_clf, X_tmp[i], y_tmp[i]) for i in range(len(X_tmp))} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "In order to evaluate the performance of the leakage detector, we compute a confusion matrix, plot the leaks as well as the alarms raised by the classifier and apply a few other metrics (as seen with the predictions).\n",
    "\n",
    "Here, we plot event (i.e. leakage) presence over time together with the raised alarms by the detector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate plots for test scenarios\n",
    "for key in scen_test_keys:\n",
    "    plt.figure()\n",
    "    plt.plot(list(range(len(y_test[key]))), y_test[key], color=\"red\", label=\"Ground truth\")\n",
    "    plt.bar(list(range(len(suspicious_time_points[key]))), suspicious_time_points[key], label=\"Raised alarm\")\n",
    "    plt.title(f\"Test Scenario {key}\")\n",
    "    plt.legend()\n",
    "    plt.ylabel(\"Leakage indicator\")\n",
    "    plt.yticks([0, 1], [\"Inactive\", \"Active\"])\n",
    "    plt.xlabel(\"Time (5min steps)\")\n",
    "    plt.show()\n",
    "\n",
    "\"\"\"# Generate plots for training scenarios\n",
    "for i, key in enumerate(scen_train_keys):\n",
    "    plt.figure()\n",
    "    plt.plot(list(range(len(y_tmp[i]))), y_tmp[i], color=\"red\", label=\"Ground truth\")\n",
    "    plt.bar(list(range(len(suspicious_time_points_train[key]))), suspicious_time_points_train[key], label=\"Raised alarm\")\n",
    "    plt.title(f\"Training Scenario {key}\")\n",
    "    plt.legend()\n",
    "    plt.ylabel(\"Leakage indicator\")\n",
    "    plt.yticks([0, 1], [\"Inactive\", \"Active\"])\n",
    "    plt.xlabel(\"Time (5min steps)\")\n",
    "    plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for scenario in scenarios.values():\n",
    "    scenario.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BattLeDIM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
