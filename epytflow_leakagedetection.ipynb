{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use-Case: Machine Learning based Leakage Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anomaly detection such as leakage detection is a classic but often non-trivial task in WDN operation. With traditional (model-based) methods reaching their limits, Machine Learning offers promising solutions.\n",
    "\n",
    "#### Outline \n",
    "This notebook demonstrates how EPyT-Flow and EPyT-Control can be utilized to create a scenario containing several leakages that have to be detected.\n",
    "Here, we use a simple Machine Learning based leakage detector that is already included in EPyT-Flow.\n",
    "It consists of the following steps:\n",
    "1. Create a new (realistic) scenario.\n",
    "2. Add some leakages to the scenario.\n",
    "3. Create a simple Machine Learning based leakage detector.\n",
    "4. Evaluate the leakage detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ImportWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from epyt_flow.data.networks import load_ltown\n",
    "from epyt_flow.data.benchmarks import load_leakdb_scenarios\n",
    "from epyt_flow.simulation import ScenarioSimulator\n",
    "from epyt_flow.simulation.events import AbruptLeakage, IncipientLeakage\n",
    "from epyt_flow.utils import to_seconds, time_points_to_one_hot_encoding\n",
    "from epyt_control.signal_processing import SensorInterpolationDetector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create new Scenario\n",
    "\n",
    "Create a new scenario based on the [L-Town network](https://epyt-flow.readthedocs.io/en/stable/epyt_flow.data.html#epyt_flow.data.networks.load_ltown) with a default sensor configuration and realistic demand patterns from the [BattLeDIM challenge](https://battledim.ucy.ac.cy/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config, = load_leakdb_scenarios(scenarios_id=[\"1\"], use_net1=False, verbose=True)\n",
    "\n",
    "scenario = ScenarioSimulator(scenario_config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set simulation duration to 2 weeks and use 5min time intervals for the hydraulics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"simulation_duration\": to_seconds(days=14),\n",
    "          \"hydraulic_time_step\": to_seconds(minutes=5),\n",
    "          \"reporting_time_step\": to_seconds(minutes=5)}\n",
    "scenario.set_general_parameters(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario.plot_topology()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topo = scenario.get_topology()\n",
    "nodes = topo.nodes\n",
    "edges = topo.edges\n",
    "print(\"Nodes: \", nodes, \"edges: \", edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Add Leakages to the Scenario\n",
    "\n",
    "In this example, we build a scenario with two leakages: A small abrupt leakage and a large incipient leakage in the second week:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "leak1 = AbruptLeakage(link_id=\"6\", diameter=0.002,\n",
    "                      start_time=to_seconds(days=7),\n",
    "                      end_time=to_seconds(days=8))\n",
    "scenario.add_leakage(leak1)\n",
    "\n",
    "leak2 = IncipientLeakage(link_id=\"19\", diameter=0.1,\n",
    "                         start_time=to_seconds(days=11),\n",
    "                         end_time=to_seconds(days=13),\n",
    "                         peak_time=to_seconds(days=12))\n",
    "scenario.add_leakage(leak2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the complete simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scada_data = scenario.run_simulation(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Machine Learning based Leakage Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the simulation results for calibrating (i.e. creating) a Machine Learning based leakage detection method:\n",
    "\n",
    "- Create a feature vector (pressure and flow readings at the sensors).\n",
    "- Create ground-truth labels utilizing the [`time_points_to_one_hot_encoding()`](https://epyt-flow.readthedocs.io/en/stable/epyt_flow.html#epyt_flow.utils.time_points_to_one_hot_encoding) helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate pressure and flow readings into a single feature vector\n",
    "X = np.concatenate((scada_data.get_data_pressures(), scada_data.get_data_flows()), axis=1)\n",
    "\n",
    "# Build ground-truth labels -- i.e. indicator of events\n",
    "events_times = [int(t / params[\"hydraulic_time_step\"])\n",
    "                for t in scenario.get_events_active_time_points()]\n",
    "y = time_points_to_one_hot_encoding(events_times, total_length=X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pressures = scada_data.get_data_pressures()\n",
    "print(type(pressures))\n",
    "print(scada_data.get_data_pressures().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_pressures = []\n",
    "for node in nodes:\n",
    "    node_pressures.append(pressures[:, int(node) - 1])\n",
    "node_pressures = np.array(node_pressures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Node pressures shape: \", node_pressures.shape)\n",
    "print(\"Node pressures: \", node_pressures)\n",
    "print(\"original pressures: \", pressures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into train and test set -- here, training data is the fault-free first week of the simulation and the second week (containing the leakages) is the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_point = 2000\n",
    "X_train, y_train = X[:split_point, :], y[:split_point]\n",
    "X_test, y_test = X[split_point:, :], y[split_point:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Machine Learning based Event Detector\n",
    "\n",
    "\n",
    "As a classic baseline, EPyT-Flow already implements a residual-based interpolation detection method called [`SensorInterpolationDetector`](https://epyt-flow.readthedocs.io/en/stable/epyt_flow.models.html#epyt_flow.models.sensor_interpolation_detector.SensorInterpolationDetector).\n",
    "\n",
    "This method tries to predict the readings of a given sensor based on all other sensors: $f: \\vec{x}_t\\setminus\\{i\\} \\mapsto (\\vec{x}_t)_i$, where $\\vec{x}_t$ refers to these sensor ratings at time $t$, and $\\vec{x}_t\\setminus\\{i\\}$ denotes these sensor readings without the $i$-th sensor.\n",
    "An alarm is raised (i.e. event detected) whenever the prediction and the observation of at least one sensor differ significantly:\n",
    "$$\n",
    "   \\exists i:\\; |f(\\vec{x}_t\\setminus\\{i\\}) - (\\vec{x}_t)_i| > \\theta_i\n",
    "$$\n",
    "where $\\theta_i > 0$ denotes a sensor-specific threshold at which the difference is considered as significant.\n",
    "For this, the detection method has to be calibrated (i.e. fitted) to a time window of (ideally event-free) sensor readings to determine a suitable threshold $\\theta$ that does not raise an alarm when the network is in normal operation (i.e. no events present).\n",
    "\n",
    "We use this event detector to detect leakages in our generated scenario.\n",
    "We create and calibrate (i.e. fit) the leakage detector to the first week of simulated data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = SensorInterpolationDetector()\n",
    "detector.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the detector to the test data (i.e. second week of simulated data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suspicious_time_points = detector.apply(X_test)\n",
    "y_test_pred = time_points_to_one_hot_encoding(suspicious_time_points, X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluation\n",
    "\n",
    "In order to evaluate the performance of the leakage detector, we could either compute the [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) or plot the raised alarms together with the ground truth labels.\n",
    "\n",
    "Here, we plot event (i.e. leakage) presence over time together with the raised alarms by the detector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(list(range(len(y_test))), y_test, color=\"red\", label=\"Ground truth\")\n",
    "plt.bar(list(range(len(y_test_pred))), y_test_pred, label=\"Raised alarm\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"Leakage indicator\")\n",
    "plt.yticks([0, 1], [\"Inactive\", \"Active\"])\n",
    "plt.xlabel(\"Time (5min steps)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the small abrupt leakage is not detected, while the large incipient leakage is detected -- only a single false alarm is raised.\n",
    "\n",
    "**Note:** More advanced algorithms & methods are likely to show a better detection performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close the Simulation\n",
    "\n",
    "Do not forget to close the simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume that we know which specific nodes we want to watch:\n",
    "def calc_Relevance (node_pressures, relevant_index):\n",
    "    relevances = {}\n",
    "    for i in range(len(node_pressures)):\n",
    "        if i != relevant_index:\n",
    "            relevances[i] = mutual_info_score(node_pressures[relevant_index], node_pressures[i])\n",
    "    return relevances\n",
    "\n",
    "# calcuate relevance over all nodes:\n",
    "\n",
    "def calc_Relevances (node_pressures):\n",
    "    relevances = {}\n",
    "    for i in range(len(node_pressures)):\n",
    "        for j in range(i + 1, len(node_pressures)):\n",
    "            m_i_score = mutual_info_score(node_pressures[i], node_pressures[j])\n",
    "            if i not in relevances:\n",
    "                relevances[i] = m_i_score\n",
    "            else:\n",
    "                relevances[i] += m_i_score\n",
    "            if j not in relevances:\n",
    "                relevances[j] = m_i_score\n",
    "            else:\n",
    "                relevances[j] += m_i_score\n",
    "    return relevances\n",
    "    \n",
    "def calc_Redundance (x_pressures, sensor_pressures):\n",
    "    redundancy = 0\n",
    "    for pressure in sensor_pressures:\n",
    "        redundancy += mutual_info_score(x_pressures, pressure)\n",
    "    redundancy /= len(sensor_pressures)\n",
    "    return redundancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_sensor_placement(node_pressures, num_sensors = 15): # do we know which nodes we want to monitor? junctions? all of them?\n",
    "    remaining_nodes = list(range(len(node_pressures)))\n",
    "    relevances = calc_Relevances(node_pressures)  # Assume we want to know the relevances in regard to all nodes\n",
    "    Sensor_placement = [max(relevances, key=relevances.get)]\n",
    "    remaining_nodes.remove(Sensor_placement[-1])\n",
    "    sensor_pressures = [node_pressures[Sensor_placement[-1]]]\n",
    "    no_redundance_nodes = [a for a in remaining_nodes if calc_Redundance(node_pressures[a], node_pressures[Sensor_placement]) == 0]\n",
    "    while no_redundance_nodes: # add nodes of highest relevance without any redundance\n",
    "        if (len(Sensor_placement) >= num_sensors):\n",
    "            break\n",
    "        tmp_relevances = relevances.copy()\n",
    "        for i in range(len(tmp_relevances.keys())): # remove nodes that have been added to the sensors and nodes with redundance\n",
    "            if i in Sensor_placement:\n",
    "                del tmp_relevances[i]\n",
    "                continue\n",
    "            if i not in no_redundance_nodes:\n",
    "                del tmp_relevances[i]\n",
    "        Sensor_placement.append(max(tmp_relevances, key=relevances.get))\n",
    "        remaining_nodes.remove(Sensor_placement[-1])\n",
    "        sensor_pressures.append(node_pressures[Sensor_placement[-1]])\n",
    "        no_redundance_nodes = [a for a in remaining_nodes if calc_Redundance(node_pressures[a], sensor_pressures) == 0]\n",
    "    remaining_relevances = list(map(relevances.get, remaining_nodes))\n",
    "    while not(all(list(map(lambda x: x == 0, remaining_relevances)))) and remaining_nodes:\n",
    "        if (len(Sensor_placement) >= num_sensors):\n",
    "            break\n",
    "        RRI = {}\n",
    "        for i in remaining_nodes:\n",
    "            RRI[i] = relevances[i]/ calc_Redundance(node_pressures[i], sensor_pressures)\n",
    "        Sensor_placement.append(max(RRI, key=RRI.get))\n",
    "        sensor_pressures.append(node_pressures[Sensor_placement[-1]])\n",
    "        remaining_nodes.remove(Sensor_placement[-1])\n",
    "        remaining_relevances = list(map(relevances.get, remaining_nodes))\n",
    "    return Sensor_placement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "placements = calc_sensor_placement(node_pressures, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sensor placements: \", placements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
