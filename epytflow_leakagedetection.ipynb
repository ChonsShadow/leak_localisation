{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Pressure sensor placement for leakage detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "(This notebook was originally based on the epyt-flow tutorial for leakage detection, which can be found here: [Tutorials](https://github.com/WaterFutures/EPyT-and-EPyT-Flow-Tutorial))\n",
    "\n",
    "**Authors:** Timo Beckmann, Tobias Kroecker\n",
    "\n",
    "This notebook documents a research project focused on evaluating two approaches to different critical aspects of leakage detection in water distribution networks:\n",
    "\n",
    "1. **Sensor Placement Strategy**: We implement and evaluate a sensor placement algorithm based on information theory. This approach aims to optimize the placement of pressure sensors by maximizing relevance and minimizing redundancy, ensuring effective network coverage. The evaluation includes varying the number of sensors to analyze the impact of sensor density on detection performance.\n",
    "\n",
    "2. **Leakage Detection Method**: A machine learning-based leakage detection approach is implemented using a bagging classifier with decision trees. This method leverages ensemble learning to enhance robustness and accuracy in detecting anomalies such as leakages in the network.\n",
    "\n",
    "3. **Datasets**: \n",
    "    - **LeakDB**: A benchmark dataset providing realistic scenarios for leakage detection in water distribution networks, used to evaluate both the sensor placement strategy and the leakage detection method.\n",
    "    - **BattLeDIM**: A dataset offering realistic demand patterns and scenarios, used to simulate and validate the sensor placement strategy.\n",
    "\n",
    "4. **Evaluation Goals**: The primary objective is to assess the performance of the leakage detection method and the effectiveness of the sensor placement strategy. This includes:\n",
    "    - Analyzing the detection accuracy of the machine learning model.\n",
    "    - Investigating the influence of different sensor placement sizes on detection performance.\n",
    "\n",
    "By combining theoretical insights from information theory and machine learning with practical applications, this notebook provides a comprehensive evaluation of the interplay between sensor placement and leakage detection in water distribution networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and method definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ImportWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import epyt_flow\n",
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from epyt_flow.data.benchmarks import leakdb\n",
    "from epyt_flow.data.benchmarks import load_leakdb_scenarios\n",
    "from epyt_flow.data.benchmarks.leakdb_data import HANOI_LEAKAGES\n",
    "from epyt_flow.data.benchmarks.battledim import load_scenario, load_scada_data\n",
    "from epyt_flow.simulation import ScenarioSimulator\n",
    "from epyt_flow.utils import to_seconds, time_points_to_one_hot_encoding\n",
    "from joblib import Parallel, delayed, parallel_backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of the sensor placement algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume that we know which specific nodes we want to watch:\n",
    "def calc_Relevance (node_pressures, relevant_index):\n",
    "    relevances = {}\n",
    "    for i in range(len(node_pressures)):\n",
    "        relevances[i] = mutual_info_score(node_pressures[relevant_index], node_pressures[i])\n",
    "    return relevances\n",
    "\n",
    "# calcuate relevance over all nodes:\n",
    "\n",
    "def calc_Relevances (node_pressures):\n",
    "    relevances = {}\n",
    "    for i in range(len(node_pressures)):\n",
    "        for j in range(i + 1, len(node_pressures)):\n",
    "            m_i_score = mutual_info_score(node_pressures[i], node_pressures[j])\n",
    "            if i not in relevances:\n",
    "                relevances[i] = m_i_score\n",
    "            else:\n",
    "                relevances[i] += m_i_score\n",
    "            if j not in relevances:\n",
    "                relevances[j] = m_i_score\n",
    "            else:\n",
    "                relevances[j] += m_i_score\n",
    "    return relevances\n",
    "    \n",
    "def calc_Redundance (x_pressures, sensor_pressures):\n",
    "    redundancy = 0\n",
    "    for pressure in sensor_pressures:\n",
    "        redundancy += mutual_info_score(x_pressures, pressure)\n",
    "    redundancy /= len(sensor_pressures)\n",
    "    return redundancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_Relevances_parallel(node_pressures, n_jobs=1):\n",
    "    relevances = Parallel(n_jobs=n_jobs)(delayed(calc_Relevance)(node_pressures, i) for i in range(len(node_pressures)))\n",
    "    return {i: sum(relevance[i] for relevance in relevances) for i in range(len(node_pressures))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_sensor_placement(node_pressures, num_sensors = 15): # do we know which nodes we want to monitor? junctions? all of them?\n",
    "    remaining_nodes = list(range(len(node_pressures)))\n",
    "    relevances = calc_Relevances(node_pressures)  # Assume we want to know the relevances in regard to all nodes\n",
    "    Sensor_placement = [max(relevances, key=relevances.get)]\n",
    "    remaining_nodes.remove(Sensor_placement[-1])\n",
    "    sensor_pressures = [node_pressures[Sensor_placement[-1]]]\n",
    "    no_redundance_nodes = [a for a in remaining_nodes if calc_Redundance(node_pressures[a], node_pressures[Sensor_placement]) == 0]\n",
    "    while no_redundance_nodes: # add nodes of highest relevance without any redundance\n",
    "        if (len(Sensor_placement) >= num_sensors):\n",
    "            break\n",
    "        tmp_relevances = relevances.copy()\n",
    "        for i in range(len(tmp_relevances.keys())): # remove nodes that have been added to the sensors and nodes with redundance\n",
    "            if i in Sensor_placement:\n",
    "                del tmp_relevances[i]\n",
    "                continue\n",
    "            if i not in no_redundance_nodes:\n",
    "                del tmp_relevances[i]\n",
    "        Sensor_placement.append(max(tmp_relevances, key=relevances.get))\n",
    "        remaining_nodes.remove(Sensor_placement[-1])\n",
    "        sensor_pressures.append(node_pressures[Sensor_placement[-1]])\n",
    "        no_redundance_nodes = [a for a in remaining_nodes if calc_Redundance(node_pressures[a], sensor_pressures) == 0]\n",
    "    remaining_relevances = list(map(relevances.get, remaining_nodes))\n",
    "    while not(all(list(map(lambda x: x == 0, remaining_relevances)))) and remaining_nodes:\n",
    "        if (len(Sensor_placement) >= num_sensors):\n",
    "            break\n",
    "        RRI = {}\n",
    "        for i in remaining_nodes:\n",
    "            RRI[i] = relevances[i]/ calc_Redundance(node_pressures[i], sensor_pressures)\n",
    "        Sensor_placement.append(max(RRI, key=RRI.get))\n",
    "        sensor_pressures.append(node_pressures[Sensor_placement[-1]])\n",
    "        remaining_nodes.remove(Sensor_placement[-1])\n",
    "        remaining_relevances = list(map(relevances.get, remaining_nodes))\n",
    "    return Sensor_placement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_sensor_placement_parallel(node_pressures, num_sensors = 15, n_jobs = 1): # do we know which nodes we want to monitor? junctions? all of them?\n",
    "    remaining_nodes = list(range(len(node_pressures)))\n",
    "    if n_jobs > 1:\n",
    "        relevances = calc_Relevances_parallel(node_pressures, n_jobs=n_jobs)\n",
    "    else:\n",
    "        relevances = calc_Relevances(node_pressures)  # Assume we want to know the relevances in regard to all nodes\n",
    "    Sensor_placement = [max(relevances, key=relevances.get)]\n",
    "    remaining_nodes.remove(Sensor_placement[-1])\n",
    "    sensor_pressures = [node_pressures[Sensor_placement[-1]]]\n",
    "    Redundances = Parallel(n_jobs=n_jobs)(delayed(calc_Redundance)(node_pressures[a], node_pressures[Sensor_placement]) for a in remaining_nodes)\n",
    "    no_redundance_nodes = [remaining_nodes[a] for a in range(len(remaining_nodes)) if Redundances[a] == 0]\n",
    "    while no_redundance_nodes: # add nodes of highest relevance without any redundance\n",
    "        if (len(Sensor_placement) >= num_sensors):\n",
    "            break\n",
    "        tmp_relevances = relevances.copy()\n",
    "        for i in range(len(tmp_relevances.keys())): # remove nodes that have been added to the sensors and nodes with redundance\n",
    "            if i in Sensor_placement:\n",
    "                del tmp_relevances[i]\n",
    "                continue\n",
    "            if i not in no_redundance_nodes:\n",
    "                del tmp_relevances[i]\n",
    "        Sensor_placement.append(max(tmp_relevances, key=relevances.get))\n",
    "        remaining_nodes.remove(Sensor_placement[-1])\n",
    "        sensor_pressures.append(node_pressures[Sensor_placement[-1]])\n",
    "        Redundances = Parallel(n_jobs=n_jobs)(delayed(calc_Redundance)(node_pressures[a], node_pressures[Sensor_placement]) for a in remaining_nodes)\n",
    "        no_redundance_nodes = [remaining_nodes[a] for a in range(len(remaining_nodes)) if Redundances[a] == 0]\n",
    "    remaining_relevances = list(map(relevances.get, remaining_nodes))\n",
    "    while not(all(list(map(lambda x: x == 0, remaining_relevances)))) and remaining_nodes:\n",
    "        if (len(Sensor_placement) >= num_sensors):\n",
    "            break\n",
    "        RRI = {}\n",
    "        for i in remaining_nodes:\n",
    "            RRI[i] = relevances[i]/ calc_Redundance(node_pressures[i], sensor_pressures)\n",
    "        Sensor_placement.append(max(RRI, key=RRI.get))\n",
    "        sensor_pressures.append(node_pressures[Sensor_placement[-1]])\n",
    "        remaining_nodes.remove(Sensor_placement[-1])\n",
    "        remaining_relevances = list(map(relevances.get, remaining_nodes))\n",
    "    return Sensor_placement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of the bagging classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DecisionTreeClassifier with entropy criterion is in the ID3 style. Classifier architecture is based on the RDTBC-SFLO paper (https://www.researchgate.net/publication/333681237_Random_Bagging_Classifier_and_Shuffled_Frog_Leaping_Based_Optimal_Sensor_Placement_for_Leakage_Detection_in_WDS)\n",
    "def mixed_model_classification_fit(n_estimators, X_train, y_train, n_jobs=1):\n",
    "    with parallel_backend('threading', n_jobs=n_jobs):\n",
    "        base = DecisionTreeClassifier(\n",
    "            criterion='entropy',\n",
    "            class_weight='balanced', \n",
    "            max_depth=5,\n",
    "            max_features='sqrt',\n",
    "            min_samples_leaf=5,\n",
    "            min_samples_split=10,\n",
    "            random_state=42\n",
    "            )\n",
    "        bagged = BaggingClassifier( #for faster runtime adjust parameters, especially n_estimators. n_jobs greatly improves runtime if set to a higher value\n",
    "            estimator=base, \n",
    "            n_estimators=n_estimators, \n",
    "            max_samples=0.8,\n",
    "            max_features=0.8,\n",
    "            bootstrap=True, \n",
    "            oob_score=True\n",
    "            ) \n",
    "        bagged.fit(X_train, y_train)\n",
    "    return bagged\n",
    "\n",
    "def mixed_model_classification_predict(classifier, X_test, y_test, n_jobs=1, print_report=False):\n",
    "    with parallel_backend('threading', n_jobs=n_jobs):\n",
    "        y_pred = classifier.predict(X_test)\n",
    "    if print_report:\n",
    "        print(classification_report(y_test, y_pred, labels=np.unique(y_test),\n",
    "                                    target_names=[f\"class_{int(c)}\" for c in np.unique(y_test)], output_dict=True))\n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# like this it can only be used for leakdb sceanrios\n",
    "def early_detection_score(X_test, y_test, y_pred, scenarios):\n",
    "    leaks_info = json.loads(HANOI_LEAKAGES)\n",
    "    ed_score = 0\n",
    "    normalizing_coef = []\n",
    "    window_tolerance = 10\n",
    "    detection_threshold = 0.75\n",
    "    for i, scen in scenarios:\n",
    "        pred = y_pred[scen]\n",
    "        leaks_time_window = leakdb.__get_leak_time_windows(scen, leaks_info)\n",
    "\n",
    "        scores = []\n",
    "        for t_0, _ in leaks_time_window:\n",
    "            normalizing_coef.append(1.)\n",
    "\n",
    "            y_pred_window = pred[t_0:t_0 + window_tolerance]\n",
    "            if 1 in y_pred_window and np.sum(y_pred_window) / len(y_pred_window) > detection_threshold:\n",
    "                t_idx = np.argwhere(y_pred_window)[0] + 1\n",
    "                scores.append(2. / (1 + np.exp((5. / window_tolerance) * t_idx)))\n",
    "            else:\n",
    "                scores.append(0.)\n",
    "        ed_score += np.sum(scores)\n",
    "    return ed_score/np.sum(normalizing_coef) if normalizing_coef else 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_window_idx(y_test):\n",
    "    diff = np.diff(y_test)\n",
    "    \n",
    "    # Find the indices where the difference is 1 (indicating a transition from 0 to 1)\n",
    "    first_one_indices = np.where(diff == 1)[0] + 1\n",
    "    \n",
    "    return first_one_indices\n",
    "\n",
    "# early detection score for that can be used with any dataset, not only leakdb\n",
    "# designed for single scenarios\n",
    "def general_early_detection_score(X_test, y_test, y_pred):\n",
    "    leaks_idx0 = find_window_idx(y_test)\n",
    "\n",
    "    ed_score = 0\n",
    "    normalizing_coef = []\n",
    "    window_tolerance = 10\n",
    "    detection_threshold = 0.75\n",
    "    for t_0 in leaks_idx0:\n",
    "        normalizing_coef.append(1.)\n",
    "\n",
    "        y_pred_window = y_pred[t_0:t_0 + window_tolerance]\n",
    "        if 1 in y_pred_window and np.sum(y_pred_window) / len(y_pred_window) > detection_threshold:\n",
    "            t_idx = np.argwhere(y_pred_window)[0] + 1\n",
    "            ed_score += 2. / (1 + np.exp((5. / window_tolerance) * t_idx))\n",
    "        else:\n",
    "            ed_score += 0.\n",
    "    return ed_score / np.sum(normalizing_coef) if normalizing_coef else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_collect(clf, X_test, y_test, verbose=False):\n",
    "    if verbose:\n",
    "        y_pred = mixed_model_classification_predict(clf, X_test, y_test, print_report=True)\n",
    "    else:\n",
    "        y_pred = mixed_model_classification_predict(clf, X_test, y_test, print_report=False)\n",
    "    rpt = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "    probabilities = clf.predict_proba(X_test)\n",
    "    uniq = np.unique(y_test)\n",
    "    if len(uniq) == 2:\n",
    "        ll  = log_loss(y_test, probabilities)\n",
    "        rc = roc_auc_score(y_test, probabilities[:,1])\n",
    "    else:\n",
    "        ll  = np.nan\n",
    "        rc = np.nan\n",
    "    early_detection_score = general_early_detection_score(X_test, y_test, y_pred)\n",
    "    df = pd.DataFrame(rpt).T[['precision','recall','f1-score']]\n",
    "    df.loc['roc_auc', :] = [rc, pd.NA, pd.NA]\n",
    "    df.loc['log_loss', :] = [ll, pd.NA, pd.NA]\n",
    "    return y_pred, df, early_detection_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = epyt_flow.data.benchmarks.leakdb.load_data(\n",
    "    #scenarios_id=[\"1\", \"2\", \"3\", \"4\", \"5\"],  \n",
    "    scenarios_id=[\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\"],  \n",
    "    #scenarios_id=[\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\"],\n",
    "    use_net1=False,\n",
    "    return_X_y=True\n",
    ")\n",
    "\n",
    "X_list, y_list = [], [] \n",
    "for sid, (Xi, yi) in data_dict.items():\n",
    "    X_list.append(Xi.values if hasattr(Xi, \"values\") else Xi) #Check for correctness. \n",
    "    y_list.append(yi)\n",
    "\n",
    "X_dummy = np.vstack(X_list)\n",
    "y_dummy = np.concatenate(y_list)\n",
    "\n",
    "print(f\"Total samples: {X_dummy.shape[0]}, features: {X_dummy.shape[1]}\")\n",
    "\n",
    "X_train_dummy, X_test_dummy, y_train_dummy, y_test_dummy = train_test_split( #Might use datasets given train-test split\n",
    "    X_dummy, y_dummy,\n",
    "    test_size=0.20,\n",
    "    random_state=42,\n",
    "    stratify=y_dummy\n",
    ")\n",
    "\n",
    "clf = mixed_model_classification_fit(100, X_train_dummy, y_train_dummy, n_jobs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sda = mixed_model_classification_predict(clf, X_test_dummy, y_test_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X20, y20 = epyt_flow.data.benchmarks.leakdb.load_data(\n",
    "    scenarios_id=[\"30\"],\n",
    "    use_net1=False,\n",
    "    return_X_y=True\n",
    ")[\"30\"]\n",
    "\n",
    "X20 = getattr(X20, \"values\", X20)\n",
    "\n",
    "y20_pred = mixed_model_classification_predict(clf, X20, y20, n_jobs=4, print_report=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeakDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Loading scenarios and placing sensors\n",
    "\n",
    "In this part a subset of the scenarios from LeakDB is loaded and based on existiong pressure data from the different scenarios, the sensor placements are calculated. The scenarios are split into a train and a testset, based on their ids. The pressure sensors are placed based on the training data, assuming that in a realistic scenario the sensors could only be placed based on a finite set of pressure data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_scens = 800\n",
    "\n",
    "#NOTE: this approach calculates a sensor placement based on multiple scenarios, i.e. all scenarios we look at\n",
    "#      due to the complexity of the algorithm this will take a lot of time!\n",
    "scen_train_keys = [str(num) for num in random.sample(range(1, 901), num_train_scens)]\n",
    "scen_test_keys = list(map(str, range(901, 1001)))\n",
    "scen_keys = scen_train_keys + scen_test_keys\n",
    "data = epyt_flow.data.benchmarks.leakdb.load_data(scenarios_id=scen_train_keys, use_net1=False, return_X_y=False)\n",
    "\n",
    "pressure_column_heads = data[scen_train_keys[0]].columns[:32]\n",
    "node_pressures = []\n",
    "for key in scen_train_keys:\n",
    "    for i in range(len(pressure_column_heads)):\n",
    "        if key == scen_train_keys[0]:\n",
    "            node_pressures.append(data[key]['Pressure-Node_' + str(i + 1)])\n",
    "        else:\n",
    "            node_pressures[i] = np.concatenate((node_pressures[i], data[key]['Pressure-Node_' + str(i + 1)]))\n",
    "node_pressures = np.array(node_pressures)\n",
    "node_pressures.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_placements = calc_sensor_placement_parallel(node_pressures, num_sensors=15, n_jobs=8)\n",
    "print(\"Sensor placements:\", sensor_placements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use the results from the bigger list of sensor placements to create a smaller list\n",
    "sensor_placements_5 = sensor_placements[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_placements_10 = sensor_placements[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = load_leakdb_scenarios(scenarios_id=scen_keys, use_net1=False, verbose=True)\n",
    "scenarios = {}\n",
    "for i, key in enumerate(scen_keys):\n",
    "    scenarios[key] = ScenarioSimulator(scenario_config=configs[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The topology of the water distribution network does not change between the different scenarios. Here the general topology is inspected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios[scen_train_keys[0]].plot_topology()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the complete set of simulations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scada_data = {}\n",
    "for key, scenario in scenarios.items():\n",
    "    scada_data[key] = scenario.run_simulation(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning based Leakage Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the simulation results for calibrating (i.e. creating) a Machine Learning based leakage detection method:\n",
    "\n",
    "- Create a feature vector (pressure readings at the sensors).\n",
    "- Create ground-truth labels utilizing the [`time_points_to_one_hot_encoding()`](https://epyt-flow.readthedocs.io/en/stable/epyt_flow.html#epyt_flow.utils.time_points_to_one_hot_encoding) helper function.\n",
    "\n",
    "The scenarios are already split into a train and test set. Because of that, two feature vectors and lable-sets are created from the start, one of each for the training of the classifier and another pair for the testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tmp = [scada_data[key].get_data_pressures()[:, sensor_placements] for key in scen_train_keys]\n",
    "X_train = np.vstack(X_tmp)\n",
    "X_test = {key: scada_data[key].get_data_pressures()[:, sensor_placements] for key in scen_test_keys}\n",
    "y_tmp = []\n",
    "y_test = {}\n",
    "for i, key in enumerate(scen_train_keys):\n",
    "    events_times = [int(t / configs[i].general_params[\"hydraulic_time_step\"])\n",
    "                    for t in scenarios[key].get_events_active_time_points()]\n",
    "    y_tmp.append(time_points_to_one_hot_encoding(events_times, total_length=X_tmp[0].shape[0]))\n",
    "for i, key in enumerate(scen_test_keys):\n",
    "    events_times = [int(t / configs[i].general_params[\"hydraulic_time_step\"])\n",
    "                    for t in scenarios[key].get_events_active_time_points()]\n",
    "    y_test[key] = time_points_to_one_hot_encoding(events_times, total_length=X_test[key].shape[0])\n",
    "y_train = np.concatenate(y_tmp)\n",
    "print(f\"Total samples: {X_train.shape[0]}, features: {X_train.shape[1]}, samples_y : {y_train.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tmp_5 = [scada_data[key].get_data_pressures()[:, sensor_placements_5] for key in scen_train_keys]\n",
    "X_train_5 = np.vstack(X_tmp_5)\n",
    "X_test_5 = {key: scada_data[key].get_data_pressures()[:, sensor_placements_5] for key in scen_test_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tmp_10 = [scada_data[key].get_data_pressures()[:, sensor_placements_10] for key in scen_train_keys]\n",
    "X_train_10 = np.vstack(X_tmp_10)\n",
    "X_test_10 = {key: scada_data[key].get_data_pressures()[:, sensor_placements_10] for key in scen_test_keys}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data is shuffled to avoid biases: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_train_5, X_train_10, y_train = sklearn.utils.shuffle(\n",
    "    X_train, X_train_5, X_train_10, y_train,\n",
    "    random_state=42\n",
    ")\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the classifier to the test data (i.e. the test scenarios). Note that one-hot-encoding isn't necessary here, since our classifier is already trained to return one prediction, 0 or 1, per timestep:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train classifiers for each different sensor size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trees = 20\n",
    "mixed_clf = mixed_model_classification_fit(num_trees, X_train, y_train, n_jobs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_clf_5 = mixed_model_classification_fit(num_trees, X_train_5, y_train, n_jobs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_clf_10 = mixed_model_classification_fit(num_trees, X_train_10, y_train, n_jobs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_path =f\"plots/LeakDB/{num_train_scens}_scens_{num_trees}_trees/{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}/\"\n",
    "if not os.path.isdir(general_path):\n",
    "    os.makedirs(general_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# —————————————————————————————————————————————————————\n",
    "# For each scenario in the test set, evaluate the model and collect metrics.\n",
    "# metrics are collected in a list of dataframes and then averaged. The result can then be plotted.\n",
    "# —————————————————————————————————————————————————————\n",
    "runs = []\n",
    "counter = 0\n",
    "alarms_plot_path = general_path + \"alarms_with_gt/\"\n",
    "if not os.path.isdir(alarms_plot_path):\n",
    "    os.makedirs(alarms_plot_path)\n",
    "ed_scores = []\n",
    "for sid in scen_test_keys:\n",
    "    counter = counter + 1\n",
    "\n",
    "    y_pred, df_metrics, early_detection_score = evaluate_and_collect(mixed_clf, X_test[sid], y_test[sid])\n",
    "\n",
    "    runs.append(df_metrics)\n",
    "    ed_scores.append(early_detection_score)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test[sid], y_pred, labels=[0,1]).ravel()\n",
    "    print(f\"Scenario {sid}: TN={tn}, FP={fp}, FN={fn}, TP={tp}\\n\")\n",
    "    \n",
    "    plt.figure(figsize=(10, 3))\n",
    "    plt.plot(y_test[sid], color=\"red\",   label=\"Ground truth\", linewidth=2)\n",
    "    plt.bar(range(len(y_pred)), y_pred, alpha=0.3, label=\"Raised alarm\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.ylabel(\"Leakage indicator\")\n",
    "    plt.yticks([0,1], [\"Inactive\",\"Active\"])\n",
    "    plt.xlabel(\"Time (5 min steps)\")\n",
    "    plt.title(f\"Scenario {sid}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(alarms_plot_path + f\"alarms_with_gt_{sid}.png\")\n",
    "    if counter <= 10:\n",
    "        plt.show() # TODO: save figure instead of showing it\n",
    "    plt.close() # TODO: is this necessary?\n",
    "df_all  = pd.concat(runs, keys=range(len(runs)), names=['run','label'])\n",
    "df_mean = df_all.groupby(level='label').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# —————————————————————————————————————————————————————\n",
    "# For the 5 sensors variation. this is a repeat of the above code but with 5 sensors instead of 15\n",
    "# —————————————————————————————————————————————————————\n",
    "runs_5 = []\n",
    "counter = 0\n",
    "alarms_plot_path = general_path + \"alarms_with_gt_5/\"\n",
    "if not os.path.isdir(alarms_plot_path):\n",
    "    os.makedirs(alarms_plot_path)\n",
    "ed_scores_5 = []\n",
    "for sid in scen_test_keys:\n",
    "    counter = counter + 1\n",
    "\n",
    "    y_pred_5, df_metrics_5, early_detection_score = evaluate_and_collect(mixed_clf_5, X_test_5[sid], y_test[sid])\n",
    "\n",
    "    runs_5.append(df_metrics_5)\n",
    "    ed_scores_5.append(early_detection_score)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test[sid], y_pred_5, labels=[0,1]).ravel()\n",
    "    print(f\"Scenario {sid}: TN={tn}, FP={fp}, FN={fn}, TP={tp}\\n\")\n",
    "    \n",
    "    plt.figure(figsize=(10, 3))\n",
    "    plt.plot(y_test[sid], color=\"red\",   label=\"Ground truth\", linewidth=2)\n",
    "    plt.bar(range(len(y_pred_5)), y_pred_5, alpha=0.3, label=\"Raised alarm\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.ylabel(\"Leakage indicator\")\n",
    "    plt.yticks([0,1], [\"Inactive\",\"Active\"])\n",
    "    plt.xlabel(\"Time (5 min steps)\")\n",
    "    plt.title(f\"Scenario {sid}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(alarms_plot_path + f\"alarms_with_gt_{sid}.png\")\n",
    "    if counter <= 10:\n",
    "        plt.show() # TODO: save figure instead of showing it\n",
    "    plt.close() # TODO: is this necessary?\n",
    "\n",
    "df_all_5  = pd.concat(runs_5, keys=range(len(runs_5)), names=['run','label'])\n",
    "df_mean_5 = df_all_5.groupby(level='label').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# —————————————————————————————————————————————————————\n",
    "# For the 10 sensors, this is a repeat of the above code but with 10 sensors instead of 15\n",
    "# —————————————————————————————————————————————————————\n",
    "runs_10 = []\n",
    "ed_scores_10 = []\n",
    "counter = 0\n",
    "alarms_plot_path = general_path + \"alarms_with_gt_10/\"\n",
    "if not os.path.isdir(alarms_plot_path):\n",
    "    os.makedirs(alarms_plot_path)\n",
    "for sid in scen_test_keys:\n",
    "    counter = counter + 1\n",
    "\n",
    "    y_pred_10, df_metrics_10, early_detection_score = evaluate_and_collect(mixed_clf_10, X_test_10[sid], y_test[sid])\n",
    "\n",
    "    runs_10.append(df_metrics_10)\n",
    "    ed_scores_10.append(early_detection_score)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test[sid], y_pred_10, labels=[0,1]).ravel()\n",
    "    print(f\"Scenario {sid}: TN={tn}, FP={fp}, FN={fn}, TP={tp}\\n\")\n",
    "\n",
    "    \n",
    "    plt.figure(figsize=(10, 3))\n",
    "    plt.plot(y_test[sid], color=\"red\",   label=\"Ground truth\", linewidth=2)\n",
    "    plt.bar(range(len(y_pred_10)), y_pred_10, alpha=0.3, label=\"Raised alarm\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.ylabel(\"Leakage indicator\")\n",
    "    plt.yticks([0,1], [\"Inactive\",\"Active\"])\n",
    "    plt.xlabel(\"Time (5 min steps)\")\n",
    "    plt.title(f\"Scenario {sid}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(alarms_plot_path + f\"alarms_with_gt_{sid}.png\")\n",
    "    if counter <= 10:\n",
    "        plt.show() # TODO: save figure instead of showing it\n",
    "    plt.close() # TODO: is this necessary?\n",
    "\n",
    "df_all_10  = pd.concat(runs_10, keys=range(len(runs_10)), names=['run','label'])\n",
    "df_mean_10 = df_all_10.groupby(level='label').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of the classifier on the LeakDB dataset\n",
    "\n",
    "In order to evaluate the performance of the leakage detector, we compute a confusion matrix, plot the leaks as well as the alarms raised by the classifier and apply a few other metrics (as seen with the predictions).\n",
    "\n",
    "Here, we plot the evaluation metrics precision, recall, f1-score, accuracy, roc_auc_score, and the log loss as an average over all test scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Disclaimer: Graphs generated using ChatGPT\n",
    "# —————————————————————————————————————————————————————\n",
    "# 1) Per‐label metrics (precision, recall, f1‑score)\n",
    "# —————————————————————————————————————————————————————\n",
    "label_metrics = ['precision', 'recall', 'f1-score']\n",
    "sensor_sets = [\n",
    "    (df_mean_5,  '5 Sensors'),\n",
    "    (df_mean_10, '10 Sensors'),\n",
    "    (df_mean,    '15 Sensors'),\n",
    "]\n",
    "\n",
    "for m in label_metrics:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 4), sharey=True)\n",
    "    fig.suptitle(f'Avg {m.capitalize()} over {len(runs_5)} Scenarios', fontsize=16)\n",
    "\n",
    "    for ax, (dfm, label) in zip(axes, sensor_sets):\n",
    "        df_plot = dfm.drop(index=[r for r in ['roc_auc','log_loss','accuracy'] if r in dfm.index])\n",
    "        bars = df_plot[m].plot(\n",
    "            kind='bar', ax=ax,\n",
    "            edgecolor='black', linewidth=1, legend=False\n",
    "        )\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.set_title(label, fontsize=14)\n",
    "        ax.set_xlabel('Class / Summary', fontsize=12)\n",
    "        if ax is axes[0]:\n",
    "            ax.set_ylabel(m.capitalize(), fontsize=12)\n",
    "        ax.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "        ax.set_xticklabels(df_plot.index, rotation=0, fontsize=10)\n",
    "\n",
    "        for rect in bars.patches:\n",
    "            h = rect.get_height()\n",
    "            ax.text(\n",
    "                rect.get_x() + rect.get_width()/2,\n",
    "                h + 0.02,\n",
    "                f'{h:.2f}',\n",
    "                ha='center', va='bottom',\n",
    "                fontsize=10\n",
    "            )\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.93])\n",
    "\n",
    "    plt.savefig(f\"{general_path}{m}.png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# —————————————————————————————————————————————————————\n",
    "# 2) Global metrics (accuracy, roc_auc, log_loss)\n",
    "# —————————————————————————————————————————————————————\n",
    "global_metrics = ['accuracy', 'roc_auc', 'log_loss']\n",
    "sensor_labels  = ['5 Sensors', '10 Sensors', '15 Sensors']\n",
    "\n",
    "# gather the “precision” column for each global metric row\n",
    "values = {\n",
    "    'accuracy': [df_mean_5.loc['accuracy','precision'],\n",
    "                 df_mean_10.loc['accuracy','precision'],\n",
    "                 df_mean.loc['accuracy','precision']],\n",
    "    'roc_auc':   [df_mean_5.loc['roc_auc','precision'],\n",
    "                  df_mean_10.loc['roc_auc','precision'],\n",
    "                  df_mean.loc['roc_auc','precision']],\n",
    "    'log_loss':  [df_mean_5.loc['log_loss','precision'],\n",
    "                  df_mean_10.loc['log_loss','precision'],\n",
    "                  df_mean.loc['log_loss','precision']],\n",
    "}\n",
    "\n",
    "x = np.arange(len(sensor_labels))\n",
    "\n",
    "for m in global_metrics:\n",
    "    fig, ax = plt.subplots(figsize=(6,4))\n",
    "    ax.bar(x, values[m], width=0.6, edgecolor='black')\n",
    "    if m in ['accuracy','roc_auc']:\n",
    "        ax.set_ylim(0, 1)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(sensor_labels)\n",
    "    ax.set_ylabel(m.replace('_',' ').capitalize())\n",
    "    ax.set_title(f'Avg {m.replace(\"_\",\" \").capitalize()} over {len(runs_5)} Scenarios')\n",
    "    for i, v in enumerate(values[m]):\n",
    "        ax.text(i, v + (0.02 if m in ['accuracy','roc_auc'] else 0),\n",
    "                f'{v:.2f}', ha='center', va='bottom')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(f\"{general_path}{m}.png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "# —————————————————————————————————————————————————————\n",
    "# 3) Early Detection Score\n",
    "# —————————————————————————————————————————————————————\n",
    "for i in range(len(ed_scores)):\n",
    "    if type(ed_scores[i]) is np.ndarray:\n",
    "        ed_scores[i] = ed_scores[i][0]\n",
    "    if type(ed_scores_5[i]) is np.ndarray:\n",
    "        ed_scores_5[i] = ed_scores_5[i][0]\n",
    "    if type(ed_scores_10[i]) is np.ndarray:\n",
    "        ed_scores_10[i] = ed_scores_10[i][0]\n",
    "mean_ed_scores = [np.mean(ed_scores_5), np.mean(ed_scores_10), np.mean(ed_scores)]\n",
    "all_scores = [ed_scores_5, ed_scores_10, ed_scores]\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\n",
    "\n",
    "\n",
    "for i, scores in enumerate(all_scores):\n",
    "    axes[i].bar(scen_test_keys[:10], scores[:10])\n",
    "    axes[i].set_title('Early Detection Score - ' + sensor_labels[i])\n",
    "    axes[i].set_xlabel('Scenario')\n",
    "    axes[i].set_ylabel('Score')\n",
    "    axes[i].set_xticks(range(len(scores[:10])))\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(f\"{general_path}early_detection_scores_scens.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(sensor_labels, mean_ed_scores)\n",
    "plt.title('Mean Scores for Each List')\n",
    "plt.xlabel('#Sensors')\n",
    "plt.ylabel('Mean Score')\n",
    "\n",
    "#scale y-axis\n",
    "plt.ylim(0, 1.0)\n",
    "\n",
    "\n",
    "plt.savefig(f\"{general_path}early_detection_scores_means.png\", dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.concat(\n",
    "    [df_mean_5, df_mean_10, df_mean],\n",
    "    axis=1,\n",
    "    keys=['5 Sensors', '10 Sensors', '15 Sensors']\n",
    ")\n",
    "\n",
    "df_combined.to_csv(f\"{general_path}metrics.csv\", index=True)\n",
    "df_ed_scores = pd.DataFrame({'scenarios': scen_test_keys, 'early_detection_score_5': ed_scores_5, 'early_detection_score_10': ed_scores_10, 'early_detection_score': ed_scores})\n",
    "df_ed_scores.to_csv(f\"{general_path}early_detection_scores.csv\", index=False)\n",
    "print(df_combined.to_markdown(floatfmt=\".3f\"))\n",
    "print(df_ed_scores.to_markdown(floatfmt=\".3f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for scenario in scenarios.values():\n",
    "    scenario.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BattLeDIM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Loading scenarios and placing sensors\n",
    "\n",
    "Here the BattLeDIM data for training and testing is loaded, the sensor placements are calculated and the simulations are run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use scada date, because data provides the pressure data for 31 nodes, while the simulation for some reason only provides it for 29\n",
    "data = load_scada_data(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_placements = calc_sensor_placement_parallel(data.get_data_pressures().T, num_sensors=29, n_jobs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_placements_20 = sensor_placements[:20]\n",
    "sensor_placements_10 = sensor_placements[:10]\n",
    "sensor_placements_5 = sensor_placements[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_scens = 10\n",
    "train_configs = []\n",
    "train_scenarios = []\n",
    "test_config = load_scenario(True)\n",
    "test_scenario = ScenarioSimulator(scenario_config=test_config)\n",
    "for i in range(num_train_scens):\n",
    "    train_configs.append(load_scenario(False))\n",
    "    train_scenarios.append(ScenarioSimulator(scenario_config=train_configs[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the simulations\n",
    "scada_data_train = [train_scenarios[i].run_simulation(verbose=True) for i in range(10)]\n",
    "scada_data_test = test_scenario.run_simulation(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning based leakage detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This includes again the preprocessing and fitting of the data from the simulations, as seen for LeakDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_BD = np.vstack([scada_data_train[i].get_data_pressures()[:, sensor_placements] for i in range(10)])\n",
    "X_train_BD_5 = np.vstack([scada_data_train[i].get_data_pressures()[:, sensor_placements_5] for i in range(10)])\n",
    "X_train_BD_10 = np.vstack([scada_data_train[i].get_data_pressures()[:, sensor_placements_10] for i in range(10)])\n",
    "X_train_BD_20 = np.vstack([scada_data_train[i].get_data_pressures()[:, sensor_placements_20] for i in range(10)])\n",
    "X_test_BD = scada_data_test.get_data_pressures()[:, sensor_placements]\n",
    "X_test_BD_5 = scada_data_test.get_data_pressures()[:, sensor_placements_5]\n",
    "X_test_BD_10 = scada_data_test.get_data_pressures()[:, sensor_placements_10]\n",
    "X_test_BD_20 = scada_data_test.get_data_pressures()[:, sensor_placements_20]\n",
    "\n",
    "y_train_BD = np.concatenate([time_points_to_one_hot_encoding(\n",
    "    [int(t / train_configs[i].general_params[\"hydraulic_time_step\"]) for t in train_scenarios[i].get_events_active_time_points()],\n",
    "    total_length=scada_data_train[i].get_data_pressures().shape[0]) for i in range(num_train_scens)])\n",
    "\n",
    "\n",
    "events_times = [int(t / test_config.general_params[\"hydraulic_time_step\"])\n",
    "                for t in test_scenario.get_events_active_time_points()]\n",
    "y_test_BD = time_points_to_one_hot_encoding(events_times, total_length=X_test_BD.shape[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the data to avoid any bias, as seen in the LeakDB part\n",
    "X_train_BD, X_train_BD_5, X_train_BD_10, X_train_BD_20, y_train_BD = sklearn.utils.shuffle(\n",
    "    X_train_BD, X_train_BD_5, X_train_BD_10, X_train_BD_20, y_train_BD,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the classifier to the data\n",
    "num_trees = 20\n",
    "mixed_clf_BD = mixed_model_classification_fit(num_trees, X_train_BD, y_train_BD, n_jobs=8)\n",
    "mixed_clf_BD_5 = mixed_model_classification_fit(num_trees, X_train_BD_5, y_train_BD, n_jobs=8)\n",
    "mixed_clf_BD_10 = mixed_model_classification_fit(num_trees, X_train_BD_10, y_train_BD, n_jobs=8)\n",
    "mixed_clf_BD_20 = mixed_model_classification_fit(num_trees, X_train_BD_20, y_train_BD, n_jobs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_suspicious_time_points_BD = mixed_model_classification_predict(mixed_clf_BD, X_train_BD, y_train_BD, n_jobs=8)\n",
    "suspicious_time_points_BD = mixed_model_classification_predict(mixed_clf_BD, X_test_BD, y_test_BD, n_jobs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "suspicious_time_points_BD_5 = mixed_model_classification_predict(mixed_clf_BD_5, X_test_BD_5, y_test_BD, n_jobs=8)\n",
    "suspicious_time_points_BD_10 = mixed_model_classification_predict(mixed_clf_BD_10, X_test_BD_10, y_test_BD, n_jobs=8)\n",
    "suspicious_time_points_BD_20 = mixed_model_classification_predict(mixed_clf_BD_20, X_test_BD_20, y_test_BD, n_jobs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = []\n",
    "ed_scores = []\n",
    "\n",
    "y_pred, df_metrics, early_detection_score = evaluate_and_collect(mixed_clf_BD, X_test_BD, y_test_BD)\n",
    "runs.append(df_metrics)\n",
    "ed_scores.append(early_detection_score)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test_BD, y_pred, labels=[0,1]).ravel()\n",
    "print(f\"test_scenario, all (29) sensors: TN={tn}, FP={fp}, FN={fn}, TP={tp}\\n\")\n",
    "\n",
    "df_all  = pd.concat(runs, keys=range(len(runs)), names=['run','label'])\n",
    "df_mean = df_all.groupby(level='label').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_5 = []\n",
    "ed_scores_5 = []\n",
    "\n",
    "y_pred_5, df_metrics_5, early_detection_score = evaluate_and_collect(mixed_clf_BD_5, X_test_BD_5, y_test_BD)\n",
    "\n",
    "runs_5.append(df_metrics_5)\n",
    "ed_scores_5.append(early_detection_score)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test_BD, y_pred_5, labels=[0,1]).ravel()\n",
    "print(f\"test_scenario, 5 sensors: TN={tn}, FP={fp}, FN={fn}, TP={tp}\\n\")\n",
    "\n",
    "df_all_5  = pd.concat(runs_5, keys=range(len(runs_5)), names=['run','label'])\n",
    "df_mean_5 = df_all_5.groupby(level='label').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_10 = []\n",
    "ed_scores_10 = []\n",
    "\n",
    "y_pred_10, df_metrics_10, early_detection_score = evaluate_and_collect(mixed_clf_BD_10, X_test_BD_10, y_test_BD)\n",
    "\n",
    "runs_10.append(df_metrics_10)\n",
    "ed_scores_10.append(early_detection_score)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test_BD, y_pred_10, labels=[0,1]).ravel()\n",
    "print(f\"test_scenario, 10 sensors: TN={tn}, FP={fp}, FN={fn}, TP={tp}\\n\")\n",
    "\n",
    "df_all_10  = pd.concat(runs_10, keys=range(len(runs_10)), names=['run','label'])\n",
    "df_mean_10 = df_all_10.groupby(level='label').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_20 = []\n",
    "ed_scores_20 = []\n",
    "\n",
    "y_pred_20, df_metrics_20, early_detection_score = evaluate_and_collect(mixed_clf_BD_20, X_test_BD_20, y_test_BD)\n",
    "\n",
    "runs_20.append(df_metrics_20)\n",
    "ed_scores_20.append(early_detection_score)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test_BD, y_pred_20, labels=[0,1]).ravel()\n",
    "print(f\"test_scenario, 20 sensors: TN={tn}, FP={fp}, FN={fn}, TP={tp}\\n\")\n",
    "\n",
    "df_all_20  = pd.concat(runs_20, keys=range(len(runs_20)), names=['run','label'])\n",
    "df_mean_20 = df_all_20.groupby(level='label').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "general_path = f\"plots/BattleDIM/{num_train_scens}_scens_{num_trees}_trees/{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}/\"\n",
    "if not os.path.isdir(general_path):\n",
    "    os.makedirs(general_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Disclaimer: Graphs generated using ChatGPT\n",
    "# —————————————————————————————————————————————————————\n",
    "# 1) Per‐label metrics (precision, recall, f1‑score)\n",
    "# —————————————————————————————————————————————————————\n",
    "label_metrics = ['precision', 'recall', 'f1-score']\n",
    "sensor_sets = [\n",
    "    (df_mean_5,  '5 Sensors'),\n",
    "    (df_mean_10, '10 Sensors'),\n",
    "    (df_mean_20, '20 Sensors'),\n",
    "    (df_mean,    '29 Sensors'),\n",
    "]\n",
    "\n",
    "for m in label_metrics:\n",
    "    fig, axes = plt.subplots(1, len(sensor_sets), figsize=(18, 4), sharey=True)\n",
    "    fig.suptitle(f'Avg {m.capitalize()} over {len(runs_5)} Scenario(s)', fontsize=16)\n",
    "\n",
    "    for ax, (dfm, label) in zip(axes, sensor_sets):\n",
    "        df_plot = dfm.drop(index=[r for r in ['roc_auc','log_loss','accuracy'] if r in dfm.index])\n",
    "        bars = df_plot[m].plot(\n",
    "            kind='bar', ax=ax,\n",
    "            edgecolor='black', linewidth=1, legend=False\n",
    "        )\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.set_title(label, fontsize=14)\n",
    "        ax.set_xlabel('Class / Summary', fontsize=12)\n",
    "        if ax is axes[0]:\n",
    "            ax.set_ylabel(m.capitalize(), fontsize=12)\n",
    "        ax.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "        ax.set_xticklabels(df_plot.index, rotation=0, fontsize=10)\n",
    "\n",
    "        for rect in bars.patches:\n",
    "            h = rect.get_height()\n",
    "            ax.text(\n",
    "                rect.get_x() + rect.get_width()/2,\n",
    "                h + 0.02,\n",
    "                f'{h:.2f}',\n",
    "                ha='center', va='bottom',\n",
    "                fontsize=10\n",
    "            )\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.93])\n",
    "\n",
    "    plt.savefig(f\"{general_path}{m}.png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# —————————————————————————————————————————————————————\n",
    "# 2) Global metrics (accuracy, roc_auc, log_loss)\n",
    "# —————————————————————————————————————————————————————\n",
    "global_metrics = ['accuracy', 'roc_auc', 'log_loss']\n",
    "sensor_labels  = ['5 Sensors', '10 Sensors', '20 Sensors', '29 Sensors']\n",
    "\n",
    "# gather the “precision” column for each global metric row\n",
    "values = {\n",
    "    'accuracy': [df_mean_5.loc['accuracy','precision'],\n",
    "                 df_mean_10.loc['accuracy','precision'],\n",
    "                 df_mean_20.loc['accuracy','precision'],\n",
    "                 df_mean.loc['accuracy','precision']],\n",
    "    'roc_auc':   [df_mean_5.loc['roc_auc','precision'],\n",
    "                  df_mean_10.loc['roc_auc','precision'],\n",
    "                  df_mean_20.loc['roc_auc','precision'],\n",
    "                  df_mean.loc['roc_auc','precision']],\n",
    "    'log_loss':  [df_mean_5.loc['log_loss','precision'],\n",
    "                  df_mean_10.loc['log_loss','precision'],\n",
    "                  df_mean_20.loc['log_loss','precision'],\n",
    "                  df_mean.loc['log_loss','precision']],\n",
    "}\n",
    "\n",
    "x = np.arange(len(sensor_labels))\n",
    "\n",
    "for m in global_metrics:\n",
    "    fig, ax = plt.subplots(figsize=(6,4))\n",
    "    ax.bar(x, values[m], width=0.6, edgecolor='black')\n",
    "    if m in ['accuracy','roc_auc']:\n",
    "        ax.set_ylim(0, 1)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(sensor_labels)\n",
    "    ax.set_ylabel(m.replace('_',' ').capitalize())\n",
    "    ax.set_title(f'Avg {m.replace(\"_\",\" \").capitalize()} over {len(runs_5)} Scenarios')\n",
    "    for i, v in enumerate(values[m]):\n",
    "        ax.text(i, v + (0.02 if m in ['accuracy','roc_auc'] else 0),\n",
    "                f'{v:.2f}', ha='center', va='bottom')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(f\"{general_path}{m}.png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "# —————————————————————————————————————————————————————\n",
    "# 3) Early Detection Score\n",
    "# —————————————————————————————————————————————————————\n",
    "\n",
    "all_scores = [ed_scores_5, ed_scores_10, ed_scores_20, ed_scores]\n",
    "all_scores = [score for sens_scores in all_scores for score in sens_scores]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(sensor_labels, all_scores)\n",
    "plt.title('Score for Each number of Sensors')\n",
    "plt.xlabel('#Sensors')\n",
    "plt.ylabel('Mean Score')\n",
    "\n",
    "#scale y-axis\n",
    "plt.ylim(0, 1.0)\n",
    "\n",
    "\n",
    "plt.savefig(f\"{general_path}early_detection_scores.png\", dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: save the metrics to a csv file and plot and save the remaining three graphs\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.plot(list(range(len(y_test_BD))), y_test_BD, color=\"red\", label=\"Ground truth\")\n",
    "plt.bar(list(range(len(suspicious_time_points_BD))), suspicious_time_points_BD, label=\"Raised alarm\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"Leakage indicator\")\n",
    "plt.yticks([0, 1], [\"Inactive\", \"Active\"])\n",
    "plt.xlabel(\"Time (5min steps)\")\n",
    "plt.savefig(f\"{general_path}ground_truth_with raised_alarms_29_sens_BD.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.plot(list(range(len(y_test_BD))), y_test_BD, color=\"red\", label=\"Ground truth\")\n",
    "plt.bar(list(range(len(suspicious_time_points_BD_20))), suspicious_time_points_BD_20, label=\"Raised alarm\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"Leakage indicator\")\n",
    "plt.yticks([0, 1], [\"Inactive\", \"Active\"])\n",
    "plt.xlabel(\"Time (5min steps)\")\n",
    "plt.savefig(f\"{general_path}ground_truth_with raised_alarms_20_sens_BD.png\", dpi=300)\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.plot(list(range(len(y_test_BD))), y_test_BD, color=\"red\", label=\"Ground truth\")\n",
    "plt.bar(list(range(len(suspicious_time_points_BD_10))), suspicious_time_points_BD_10, label=\"Raised alarm\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"Leakage indicator\")\n",
    "plt.yticks([0, 1], [\"Inactive\", \"Active\"])\n",
    "plt.xlabel(\"Time (5min steps)\")\n",
    "plt.savefig(f\"{general_path}ground_truth_with raised_alarms_10_sens_BD.png\", dpi=300)\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.plot(list(range(len(y_test_BD))), y_test_BD, color=\"red\", label=\"Ground truth\")\n",
    "plt.bar(list(range(len(suspicious_time_points_BD_5))), suspicious_time_points_BD_5, label=\"Raised alarm\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"Leakage indicator\")\n",
    "plt.yticks([0, 1], [\"Inactive\", \"Active\"])\n",
    "plt.xlabel(\"Time (5min steps)\")\n",
    "plt.savefig(f\"{general_path}ground_truth_with raised_alarms_5_sens_BD.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.concat(\n",
    "    [df_mean_5, df_mean_10, df_mean_20, df_mean],\n",
    "    axis=1,\n",
    "    keys=['5 Sensors', '10 Sensors', '20 Sensors', '29 Sensors']\n",
    ")\n",
    "\n",
    "df_combined.to_csv(f\"{general_path}metrics.csv\", index=True)\n",
    "df_ed_scores = pd.DataFrame({'scenarios': ['test_scenario'], 'early_detection_score_5': ed_scores_5, 'early_detection_score_10': ed_scores_10, 'early_detection_score_20': ed_scores_20, 'early_detection_score': ed_scores})\n",
    "df_ed_scores.to_csv(f\"{general_path}early_detection_scores.csv\", index=False)\n",
    "print(df_combined.to_markdown(floatfmt=\".3f\"))\n",
    "print(df_ed_scores.to_markdown(floatfmt=\".3f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "for scen in train_scenarios:\n",
    "    scen.close()\n",
    "test_scenario.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
